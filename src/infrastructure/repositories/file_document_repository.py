#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ä»¶ç³»ç»Ÿæ–‡æ¡£ä»“å‚¨å®ç°

åŸºäºæ–‡ä»¶ç³»ç»Ÿçš„æ–‡æ¡£æ•°æ®æŒä¹…åŒ–å®ç°
"""

import json
import re
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime

from src.domain.entities.document import Document, DocumentType, DocumentStatus, create_document
from src.domain.repositories.document_repository import IDocumentRepository
from src.shared.utils.logger import get_logger
from src.shared.utils.cache_manager import get_cache_manager

logger = get_logger(__name__)


class FileDocumentRepository(IDocumentRepository):
    """
    æ–‡ä»¶ç³»ç»Ÿæ–‡æ¡£ä»“å‚¨å®ç°

    åŸºäºæ–‡ä»¶ç³»ç»Ÿçš„æ–‡æ¡£æ•°æ®æŒä¹…åŒ–å®ç°ï¼Œä½¿ç”¨JSONæ ¼å¼å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®ï¼Œ
    ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶å­˜å‚¨æ–‡æ¡£å†…å®¹ã€‚

    å®ç°æ–¹å¼ï¼š
    - ä½¿ç”¨JSONæ–‡ä»¶å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
    - ä½¿ç”¨ç‹¬ç«‹çš„æ–‡æœ¬æ–‡ä»¶å­˜å‚¨æ–‡æ¡£å†…å®¹
    - æ”¯æŒè·¨é¡¹ç›®çš„æ–‡æ¡£æŸ¥æ‰¾
    - æä¾›å®Œæ•´çš„CRUDæ“ä½œ
    - åŒ…å«æ–‡æ¡£å†…å®¹çš„æœç´¢åŠŸèƒ½

    Attributes:
        base_path: æ–‡æ¡£å­˜å‚¨çš„åŸºç¡€è·¯å¾„
    """

    def __init__(self, base_path: Optional[Path] = None):
        """
        åˆå§‹åŒ–æ–‡ä»¶ç³»ç»Ÿæ–‡æ¡£ä»“å‚¨

        Args:
            base_path: æ–‡æ¡£å­˜å‚¨çš„åŸºç¡€è·¯å¾„ï¼Œé»˜è®¤ä¸ºç”¨æˆ·ç›®å½•ä¸‹çš„.novel_editor/documents
        """
        self.base_path = base_path or Path.home() / ".novel_editor" / "documents"
        self.base_path.mkdir(parents=True, exist_ok=True)

        # ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†å™¨
        self._cache_manager = get_cache_manager()

        # ç¼“å­˜é”®å‰ç¼€
        self._cache_prefix = "doc_repo"

    def _get_document_path(self, document_id: str) -> Path:
        """
        è·å–æ–‡æ¡£å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„

        Args:
            document_id: æ–‡æ¡£å”¯ä¸€æ ‡è¯†ç¬¦

        Returns:
            Path: æ–‡æ¡£å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        return self.base_path / f"{document_id}.json"

    def _get_content_path(self, document_id: str) -> Path:
        """
        è·å–æ–‡æ¡£å†…å®¹æ–‡ä»¶è·¯å¾„

        Args:
            document_id: æ–‡æ¡£å”¯ä¸€æ ‡è¯†ç¬¦

        Returns:
            Path: æ–‡æ¡£å†…å®¹æ–‡ä»¶è·¯å¾„
        """
        return self.base_path / f"{document_id}_content.txt"

    async def _find_document_in_projects(self, document_id: str) -> tuple[Optional[Path], Optional[Path]]:
        """åœ¨æ‰€æœ‰é¡¹ç›®ç›®å½•ä¸­æŸ¥æ‰¾æ–‡æ¡£ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""
        try:
            # æ£€æŸ¥ç»Ÿä¸€ç¼“å­˜
            cache_key = f"{self._cache_prefix}:doc_paths:{document_id}"
            cached_paths = self._cache_manager.get(cache_key)

            if cached_paths:
                if cached_paths[0] and cached_paths[0].exists():
                    logger.debug(f"âš¡ ä»ç¼“å­˜ä¸­æ‰¾åˆ°æ–‡æ¡£: {cached_paths[0]}")
                    return cached_paths
                else:
                    # ç¼“å­˜çš„è·¯å¾„ä¸å­˜åœ¨ï¼Œç§»é™¤ç¼“å­˜
                    self._cache_manager.delete(cache_key)

            from config.settings import get_settings
            settings = get_settings()

            # æ£€æŸ¥é¡¹ç›®ç´¢å¼•
            projects_dir = settings.data_dir / "projects"
            index_file = projects_dir / "projects_index.json"

            if index_file.exists():
                import json
                with open(index_file, 'r', encoding='utf-8') as f:
                    index = json.load(f)

                # åœ¨æ¯ä¸ªé¡¹ç›®çš„documentsç›®å½•ä¸­æŸ¥æ‰¾
                for project_id, project_info in index.items():
                    project_path_str = project_info.get('path')
                    if project_path_str:
                        project_path = Path(project_path_str)
                        docs_dir = project_path / "documents"

                        if docs_dir.exists():
                            doc_path = docs_dir / f"{document_id}.json"
                            content_path = docs_dir / f"{document_id}_content.txt"

                            if doc_path.exists():
                                logger.debug(f"ğŸ” åœ¨é¡¹ç›® {project_id} ä¸­æ‰¾åˆ°æ–‡æ¡£: {doc_path}")

                                # ç¼“å­˜ç»“æœåˆ°ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
                                cache_key = f"{self._cache_prefix}:doc_paths:{document_id}"
                                self._cache_manager.set(cache_key, (doc_path, content_path), ttl=300)

                                return doc_path, content_path

            return None, None

        except Exception as e:
            logger.error(f"åœ¨é¡¹ç›®ä¸­æŸ¥æ‰¾æ–‡æ¡£å¤±è´¥: {e}")
            return None, None

    async def _get_document_save_path(self, document: Document) -> Path:
        """è·å–æ–‡æ¡£ä¿å­˜è·¯å¾„"""
        logger.debug(f"è·å–æ–‡æ¡£ä¿å­˜è·¯å¾„ï¼Œé¡¹ç›®ID: {document.project_id}")

        # å¦‚æœæ–‡æ¡£æœ‰é¡¹ç›®IDï¼Œå°è¯•åœ¨é¡¹ç›®ç›®å½•ä¸‹ä¿å­˜
        if document.project_id:
            try:
                # æ–¹æ³•1: æ£€æŸ¥å…¨å±€ç´¢å¼•
                from config.settings import get_settings
                settings = get_settings()

                # ç¡®ä¿projectsç›®å½•å­˜åœ¨
                projects_dir = settings.data_dir / "projects"
                projects_dir.mkdir(parents=True, exist_ok=True)

                index_file = projects_dir / "projects_index.json"
                logger.debug(f"æ£€æŸ¥ç´¢å¼•æ–‡ä»¶: {index_file}")

                if index_file.exists():
                    import json
                    with open(index_file, 'r', encoding='utf-8') as f:
                        index = json.load(f)

                    logger.debug(f"ç´¢å¼•ä¸­çš„é¡¹ç›®æ•°é‡: {len(index)}")
                    project_info = index.get(document.project_id)

                    if project_info:
                        logger.debug(f"æ‰¾åˆ°é¡¹ç›®ä¿¡æ¯: {project_info}")

                        # å°è¯•è·å–é¡¹ç›®è·¯å¾„ï¼Œæ”¯æŒå¤šç§å­—æ®µå
                        project_path_str = project_info.get('path') or project_info.get('file_path')

                        # å¦‚æœæ˜¯file_pathï¼Œéœ€è¦è·å–å…¶çˆ¶ç›®å½•
                        if project_path_str:
                            project_path = Path(project_path_str)

                            # å¦‚æœæ˜¯file_pathï¼ˆæŒ‡å‘JSONæ–‡ä»¶ï¼‰ï¼Œè·å–å…¶çˆ¶ç›®å½•
                            if project_path_str == project_info.get('file_path') and project_path.suffix == '.json':
                                # è¿™æ˜¯ä¸€ä¸ªJSONæ–‡ä»¶è·¯å¾„ï¼Œä¸æ˜¯é¡¹ç›®ç›®å½•è·¯å¾„
                                # å¯¹äºè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬ä½¿ç”¨é»˜è®¤è·¯å¾„
                                logger.debug(f"é¡¹ç›®å­˜å‚¨ä¸ºJSONæ–‡ä»¶: {project_path}ï¼Œä½¿ç”¨é»˜è®¤æ–‡æ¡£è·¯å¾„")
                            else:
                                # è¿™æ˜¯ä¸€ä¸ªé¡¹ç›®ç›®å½•è·¯å¾„
                                logger.debug(f"é¡¹ç›®è·¯å¾„: {project_path}")

                                if project_path.exists():
                                    # åœ¨é¡¹ç›®ç›®å½•ä¸‹åˆ›å»ºdocumentså­ç›®å½•
                                    documents_path = project_path / "documents"
                                    documents_path.mkdir(parents=True, exist_ok=True)
                                    logger.info(f"æ–‡æ¡£å°†ä¿å­˜åˆ°é¡¹ç›®ç›®å½•: {documents_path}")
                                    return documents_path
                                else:
                                    logger.debug(f"é¡¹ç›®è·¯å¾„ä¸å­˜åœ¨: {project_path}")

                            # æ£€æŸ¥æ˜¯å¦æœ‰ 'path' å­—æ®µï¼ˆé¡¹ç›®ç›®å½•è·¯å¾„ï¼‰
                            if 'path' in project_info and project_info['path']:
                                project_dir_path = Path(project_info['path'])
                                if project_dir_path.exists():
                                    documents_path = project_dir_path / "documents"
                                    documents_path.mkdir(parents=True, exist_ok=True)
                                    logger.info(f"æ–‡æ¡£å°†ä¿å­˜åˆ°é¡¹ç›®ç›®å½•: {documents_path}")
                                    return documents_path
                        else:
                            logger.warning(f"é¡¹ç›®ä¿¡æ¯ä¸­æ²¡æœ‰è·¯å¾„å­—æ®µ")
                    else:
                        logger.warning(f"ç´¢å¼•ä¸­æœªæ‰¾åˆ°é¡¹ç›®: {document.project_id}")
                else:
                    logger.warning(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file}")

                # æ–¹æ³•2: åœ¨é»˜è®¤é¡¹ç›®ç›®å½•ä¸­æŸ¥æ‰¾
                default_project_path = projects_dir / document.project_id
                logger.debug(f"å°è¯•é»˜è®¤é¡¹ç›®è·¯å¾„: {default_project_path}")

                if default_project_path.exists():
                    documents_path = default_project_path / "documents"
                    documents_path.mkdir(parents=True, exist_ok=True)
                    logger.info(f"æ–‡æ¡£å°†ä¿å­˜åˆ°é»˜è®¤é¡¹ç›®ç›®å½•: {documents_path}")
                    return documents_path

            except Exception as e:
                logger.error(f"è·å–é¡¹ç›®è·¯å¾„å¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())

        # ä½¿ç”¨é»˜è®¤è·¯å¾„
        logger.info(f"ä½¿ç”¨é»˜è®¤æ–‡æ¡£ç›®å½•: {self.base_path}")
        logger.debug(f"é¡¹ç›® {document.project_id} çš„æ–‡æ¡£å°†ä¿å­˜åˆ°é»˜è®¤ä½ç½®ï¼Œè¿™æ˜¯æ­£å¸¸çš„")
        return self.base_path
    
    async def save(self, document: Document) -> bool:
        """ä¿å­˜æ–‡æ¡£ï¼ˆå¸¦ç¼“å­˜æ¸…ç†ï¼‰"""
        doc_temp_file = None
        content_temp_file = None
        try:
            # ç¡®å®šä¿å­˜è·¯å¾„
            save_path = await self._get_document_save_path(document)

            # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®
            doc_path = save_path / f"{document.id}.json"
            doc_temp_file = doc_path.with_suffix('.tmp')
            doc_data = document.to_dict()

            # åˆ†ç¦»å†…å®¹å’Œå…ƒæ•°æ®
            content = doc_data.pop('content', '')

            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¡®ä¿åŸå­æ€§å†™å…¥
            with open(doc_temp_file, 'w', encoding='utf-8') as f:
                json.dump(doc_data, f, indent=2, ensure_ascii=False)

            # éªŒè¯å†™å…¥çš„å…ƒæ•°æ®æ–‡ä»¶
            with open(doc_temp_file, 'r', encoding='utf-8') as f:
                json.load(f)

            # ä¿å­˜æ–‡æ¡£å†…å®¹
            content_path = save_path / f"{document.id}_content.txt"
            content_temp_file = content_path.with_suffix('.tmp')

            with open(content_temp_file, 'w', encoding='utf-8') as f:
                f.write(content or '')

            # åŸå­æ€§æ›¿æ¢
            doc_temp_file.replace(doc_path)
            content_temp_file.replace(content_path)

            # åˆ›å»ºç‰ˆæœ¬å¤‡ä»½ï¼ˆå¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼‰
            if content and len(content.strip()) > 0:
                try:
                    version_id = await self.create_version(
                        document.id,
                        content,
                        f"è‡ªåŠ¨ä¿å­˜ç‰ˆæœ¬ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                    )
                    if version_id:
                        logger.debug(f"åˆ›å»ºç‰ˆæœ¬å¤‡ä»½: {document.id} -> {version_id}")
                except Exception as e:
                    logger.warning(f"åˆ›å»ºç‰ˆæœ¬å¤‡ä»½å¤±è´¥: {e}")
                    # ç‰ˆæœ¬åˆ›å»ºå¤±è´¥ä¸å½±å“æ–‡æ¡£ä¿å­˜

            # æ¸…ç†ç›¸å…³ç¼“å­˜
            self._clear_project_cache(document.project_id)

            logger.info(f"æ–‡æ¡£ä¿å­˜æˆåŠŸ: {document.title} ({document.id})")
            return True

        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if doc_temp_file and doc_temp_file.exists():
                try:
                    doc_temp_file.unlink()
                except Exception:
                    pass
            if content_temp_file and content_temp_file.exists():
                try:
                    content_temp_file.unlink()
                except Exception:
                    pass
            logger.error(f"ä¿å­˜æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    async def load(self, document_id: str) -> Optional[Document]:
        """æ ¹æ®IDåŠ è½½æ–‡æ¡£ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            import time
            start_time = time.time()

            # é¦–å…ˆå°è¯•ä»é»˜è®¤è·¯å¾„åŠ è½½
            doc_path = self._get_document_path(document_id)
            content_path = self._get_content_path(document_id)

            # å¦‚æœé»˜è®¤è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨æ‰€æœ‰é¡¹ç›®ç›®å½•ä¸­æŸ¥æ‰¾
            if not doc_path.exists():
                doc_path, content_path = await self._find_document_in_projects(document_id)
                if not doc_path or not doc_path.exists():
                    return None

            # åŠ è½½å…ƒæ•°æ®
            try:
                with open(doc_path, 'r', encoding='utf-8') as f:
                    doc_data = json.load(f)

                # éªŒè¯æ•°æ®æ ¼å¼
                if not isinstance(doc_data, dict):
                    logger.error(f"æ–‡æ¡£å…ƒæ•°æ®æ ¼å¼æ— æ•ˆ: {doc_path}")
                    return None

            except (json.JSONDecodeError, UnicodeDecodeError) as e:
                logger.error(f"æ–‡æ¡£å…ƒæ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯ {doc_path}: {e}")
                return None

            # åŠ è½½å†…å®¹
            content = ""
            if content_path and content_path.exists():
                try:
                    with open(content_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError as e:
                    logger.warning(f"æ–‡æ¡£å†…å®¹ç¼–ç é”™è¯¯ {content_path}: {e}")
                    # å°è¯•å…¶ä»–ç¼–ç 
                    try:
                        with open(content_path, 'r', encoding='gbk') as f:
                            content = f.read()
                    except Exception:
                        content = ""

            doc_data['content'] = content
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            document_type = DocumentType(doc_data.get('type', 'chapter'))
            document = create_document(
                document_type=document_type,
                title=doc_data['metadata']['title'],
                document_id=doc_data['id'],
                content=content,
                status=DocumentStatus(doc_data.get('status', 'draft')),
                project_id=doc_data.get('project_id')
            )
            
            # æ¢å¤å…¶ä»–å±æ€§
            if 'metadata' in doc_data:
                metadata = doc_data['metadata']
                document.metadata.description = metadata.get('description', '')
                document.metadata.tags = set(metadata.get('tags', []))
                document.metadata.author = metadata.get('author', '')
                if metadata.get('created_at'):
                    document.metadata.created_at = datetime.fromisoformat(metadata['created_at'])
                if metadata.get('updated_at'):
                    document.metadata.updated_at = datetime.fromisoformat(metadata['updated_at'])
            
            load_time = time.time() - start_time
            logger.info(f"âš¡ æ–‡æ¡£åŠ è½½æˆåŠŸ: {document.title} ({document.id}) - è€—æ—¶: {load_time:.3f}s")
            return document
            
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            return None
    
    async def delete(self, document_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        try:
            doc_path = self._get_document_path(document_id)
            content_path = self._get_content_path(document_id)
            
            if doc_path.exists():
                doc_path.unlink()
            
            if content_path.exists():
                content_path.unlink()
            
            logger.info(f"æ–‡æ¡£åˆ é™¤æˆåŠŸ: {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    async def exists(self, document_id: str) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨"""
        doc_path = self._get_document_path(document_id)
        return doc_path.exists()
    
    async def list_by_project(self, project_id: str) -> List[Document]:
        """åˆ—å‡ºé¡¹ç›®ä¸­çš„æ‰€æœ‰æ–‡æ¡£ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            import time
            start_time = time.time()

            logger.info(f"ğŸ“‹ å¼€å§‹è·å–é¡¹ç›®æ–‡æ¡£åˆ—è¡¨: {project_id}")

            # ä½¿ç”¨ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
            cache_key = f"{self._cache_prefix}:project_docs:{project_id}"
            cached_documents = self._cache_manager.get(cache_key)
            if cached_documents:
                logger.info(f"âš¡ ä»ç¼“å­˜è·å–é¡¹ç›®æ–‡æ¡£: {len(cached_documents)} ä¸ª")
                return cached_documents

            documents = []
            found_doc_ids = set()

            # ä¼˜åŒ–çš„æ–‡æ¡£æŸ¥æ‰¾ç­–ç•¥
            search_paths = await self._get_project_document_paths(project_id)

            for search_path in search_paths:
                if not search_path.exists():
                    continue

                logger.debug(f"ğŸ” æœç´¢è·¯å¾„: {search_path}")

                # æ‰¹é‡è¯»å–æ–‡æ¡£å…ƒæ•°æ®ï¼Œé¿å…é€ä¸ªåŠ è½½å®Œæ•´æ–‡æ¡£
                doc_files = list(search_path.glob("*.json"))
                logger.debug(f"ğŸ“„ æ‰¾åˆ° {len(doc_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")

                for doc_file in doc_files:
                    try:
                        # åªè¯»å–å…ƒæ•°æ®ï¼Œä¸åŠ è½½å†…å®¹
                        with open(doc_file, 'r', encoding='utf-8') as f:
                            doc_data = json.load(f)

                        # éªŒè¯æ–‡æ¡£æ•°æ®çš„åŸºæœ¬ç»“æ„
                        if not self._validate_document_data(doc_data, project_id):
                            continue

                        if doc_data.get('id') not in found_doc_ids:
                            # åˆ›å»ºè½»é‡çº§æ–‡æ¡£å¯¹è±¡ï¼ˆä¸åŠ è½½å†…å®¹ï¼‰
                            document = await self._create_lightweight_document(doc_data)
                            if document:
                                documents.append(document)
                                found_doc_ids.add(document.id)
                                logger.debug(f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£: {document.title}")

                    except json.JSONDecodeError as e:
                        logger.warning(f"JSONæ ¼å¼é”™è¯¯: {doc_file}, {e}")
                    except Exception as e:
                        logger.warning(f"è¯»å–æ–‡æ¡£å…ƒæ•°æ®å¤±è´¥: {doc_file}, {e}")

            # ç¼“å­˜ç»“æœåˆ°ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
            cache_key = f"{self._cache_prefix}:project_docs:{project_id}"
            self._cache_manager.set(cache_key, documents, ttl=60)  # 1åˆ†é’Ÿç¼“å­˜

            load_time = time.time() - start_time
            logger.info(f"âš¡ é¡¹ç›®æ–‡æ¡£åˆ—è¡¨è·å–å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£, è€—æ—¶: {load_time:.3f}s")

            return documents

        except Exception as e:
            logger.error(f"âŒ è·å–é¡¹ç›®æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def _get_project_document_paths(self, project_id: str) -> List[Path]:
        """è·å–é¡¹ç›®æ–‡æ¡£çš„æœç´¢è·¯å¾„"""
        try:
            paths = []

            # 1. é¡¹ç›®ç‰¹å®šè·¯å¾„
            try:
                from src.domain.entities.document import Document, DocumentType
                temp_doc = Document(
                    title="temp",
                    document_type=DocumentType.CHAPTER,
                    project_id=project_id
                )
                project_docs_path = await self._get_document_save_path(temp_doc)
                paths.append(project_docs_path)
            except Exception as e:
                logger.debug(f"è·å–é¡¹ç›®ç‰¹å®šè·¯å¾„å¤±è´¥: {e}")

            # 2. é»˜è®¤æ–‡æ¡£ç›®å½•
            paths.append(self.base_path)

            return paths

        except Exception as e:
            logger.error(f"è·å–é¡¹ç›®æ–‡æ¡£è·¯å¾„å¤±è´¥: {e}")
            return [self.base_path]

    def _validate_document_data(self, doc_data: dict, project_id: str) -> bool:
        """éªŒè¯æ–‡æ¡£æ•°æ®çš„åŸºæœ¬ç»“æ„"""
        try:
            # æ£€æŸ¥åŸºæœ¬å­—æ®µ
            if not isinstance(doc_data, dict):
                return False

            # æ£€æŸ¥ID
            if not doc_data.get('id'):
                logger.debug("æ–‡æ¡£æ•°æ®ç¼ºå°‘IDå­—æ®µ")
                return False

            # æ£€æŸ¥é¡¹ç›®IDåŒ¹é…
            doc_project_id = doc_data.get('project_id')
            if doc_project_id != project_id:
                logger.debug(f"é¡¹ç›®IDä¸åŒ¹é…: æœŸæœ› {project_id}, å®é™… {doc_project_id}")
                return False

            # æ£€æŸ¥æ–‡æ¡£ç±»å‹
            doc_type = doc_data.get('type') or doc_data.get('document_type')
            if not doc_type:
                logger.debug("æ–‡æ¡£æ•°æ®ç¼ºå°‘ç±»å‹å­—æ®µ")
                return False

            return True

        except Exception as e:
            logger.debug(f"éªŒè¯æ–‡æ¡£æ•°æ®å¤±è´¥: {e}")
            return False

    async def _create_lightweight_document(self, doc_data: dict):
        """åˆ›å»ºè½»é‡çº§æ–‡æ¡£å¯¹è±¡ï¼ˆä¸åŠ è½½å†…å®¹ï¼‰"""
        try:
            from src.domain.entities.document import Document, DocumentType

            # éªŒè¯åŸºæœ¬å¿…è¦å­—æ®µ
            if 'id' not in doc_data:
                logger.debug(f"æ–‡æ¡£æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: id")
                return None

            # æ£€æŸ¥æ–‡æ¡£ç±»å‹å­—æ®µï¼ˆå¯èƒ½æ˜¯typeæˆ–document_typeï¼‰
            doc_type_value = doc_data.get('type') or doc_data.get('document_type')
            if not doc_type_value:
                logger.debug(f"æ–‡æ¡£æ•°æ®ç¼ºå°‘æ–‡æ¡£ç±»å‹å­—æ®µ")
                return None

            # å®‰å…¨è½¬æ¢æ–‡æ¡£ç±»å‹
            try:
                if isinstance(doc_type_value, str):
                    doc_type = DocumentType(doc_type_value)
                else:
                    doc_type = DocumentType.CHAPTER
            except ValueError:
                logger.debug(f"æ— æ•ˆçš„æ–‡æ¡£ç±»å‹: {doc_type_value}, ä½¿ç”¨é»˜è®¤ç±»å‹")
                doc_type = DocumentType.CHAPTER

            # æ£€æŸ¥æ ‡é¢˜ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            title = self._extract_document_title(doc_data)

            # æå–å…ƒæ•°æ®
            metadata = self._extract_document_metadata(doc_data, title)

            # æå–ç»Ÿè®¡ä¿¡æ¯
            statistics = self._extract_document_statistics(doc_data)

            # ç¡®ä¿æ•°æ®ç»“æ„æ­£ç¡®
            normalized_data = {
                'id': doc_data['id'],
                'type': doc_type.value,
                'content': '',  # è½»é‡çº§å¯¹è±¡ä¸åŠ è½½å†…å®¹
                'project_id': doc_data.get('project_id', ''),
                'metadata': metadata,
                'statistics': statistics,
                'status': doc_data.get('status', 'draft'),
                'type_specific_data': doc_data.get('type_specific_data', {})
            }

            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            document = Document.from_dict(normalized_data)

            return document

        except Exception as e:
            logger.debug(f"åˆ›å»ºè½»é‡çº§æ–‡æ¡£å¯¹è±¡å¤±è´¥: {e}")
            return None

    def _extract_document_title(self, doc_data: dict) -> str:
        """æå–æ–‡æ¡£æ ‡é¢˜ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰"""
        # æ–¹æ³•1: ä»metadataä¸­è·å–
        if 'metadata' in doc_data and isinstance(doc_data['metadata'], dict):
            title = doc_data['metadata'].get('title', '')
            if title:
                return title

        # æ–¹æ³•2: ä»é¡¶çº§å­—æ®µè·å–
        title = doc_data.get('title', '')
        if title:
            return title

        # æ–¹æ³•3: ä»nameå­—æ®µè·å–ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
        title = doc_data.get('name', '')
        if title:
            return title

        # æ–¹æ³•4: ä½¿ç”¨é»˜è®¤æ ‡é¢˜
        doc_id = doc_data.get('id', 'unknown')
        return f"æ–‡æ¡£_{doc_id[:8]}"

    def _extract_document_metadata(self, doc_data: dict, title: str) -> dict:
        """æå–æ–‡æ¡£å…ƒæ•°æ®"""
        metadata = {
            'title': title,
            'description': '',
            'tags': [],
            'author': '',
            'created_at': '',
            'updated_at': ''
        }

        # ä»metadataå­—æ®µæå–
        if 'metadata' in doc_data and isinstance(doc_data['metadata'], dict):
            source_metadata = doc_data['metadata']
            metadata.update({
                'description': source_metadata.get('description', ''),
                'tags': source_metadata.get('tags', []),
                'author': source_metadata.get('author', ''),
                'created_at': source_metadata.get('created_at', ''),
                'updated_at': source_metadata.get('updated_at', '')
            })

        # å…¼å®¹æ—§æ ¼å¼çš„é¡¶çº§å­—æ®µ
        if not metadata['created_at']:
            metadata['created_at'] = doc_data.get('created_at', '')
        if not metadata['updated_at']:
            metadata['updated_at'] = doc_data.get('updated_at', '')

        return metadata

    def _extract_document_statistics(self, doc_data: dict) -> dict:
        """æå–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        default_stats = {
            'word_count': 0,
            'character_count': 0,
            'paragraph_count': 0,
            'sentence_count': 0,
            'reading_time_minutes': 0.0
        }

        # ä»statisticså­—æ®µæå–
        if 'statistics' in doc_data and isinstance(doc_data['statistics'], dict):
            stats = doc_data['statistics']
            default_stats.update({
                'word_count': max(0, stats.get('word_count', 0)),
                'character_count': max(0, stats.get('character_count', 0)),
                'paragraph_count': max(0, stats.get('paragraph_count', 0)),
                'sentence_count': max(0, stats.get('sentence_count', 0)),
                'reading_time_minutes': max(0.0, stats.get('reading_time_minutes', 0.0))
            })

        return default_stats

    def _clear_project_cache(self, project_id: str) -> None:
        """æ¸…ç†æŒ‡å®šé¡¹ç›®çš„ç¼“å­˜"""
        try:
            # æ¸…ç†é¡¹ç›®æ–‡æ¡£ç¼“å­˜
            cache_key = f"{self._cache_prefix}:project_docs:{project_id}"
            self._cache_manager.delete(cache_key)
            logger.debug(f"âœ… å·²æ¸…ç†é¡¹ç›®æ–‡æ¡£ç¼“å­˜: {project_id}")

        except Exception as e:
            logger.debug(f"æ¸…ç†é¡¹ç›®ç¼“å­˜å¤±è´¥: {e}")

    def clear_all_cache(self) -> None:
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        try:
            # æ³¨æ„ï¼šè¿™é‡Œåªèƒ½æ¸…ç†æˆ‘ä»¬çŸ¥é“çš„ç¼“å­˜é”®
            # ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨çš„clear()ä¼šæ¸…ç†æ‰€æœ‰ç¼“å­˜ï¼Œå¯èƒ½å½±å“å…¶ä»–ç»„ä»¶
            logger.debug("âœ… æ–‡æ¡£ä»“å‚¨ç¼“å­˜å·²é€šè¿‡ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨ç®¡ç†")

        except Exception as e:
            logger.debug(f"æ¸…ç†æ‰€æœ‰ç¼“å­˜å¤±è´¥: {e}")
    
    async def list_by_type(
        self, 
        document_type: DocumentType, 
        project_id: Optional[str] = None
    ) -> List[Document]:
        """æ ¹æ®ç±»å‹åˆ—å‡ºæ–‡æ¡£"""
        documents = []
        
        for doc_file in self.base_path.glob("*.json"):
            try:
                with open(doc_file, 'r', encoding='utf-8') as f:
                    doc_data = json.load(f)
                
                if (doc_data.get('document_type') == document_type.value and
                    (project_id is None or doc_data.get('project_id') == project_id)):
                    document = await self.load(doc_data['id'])
                    if document:
                        documents.append(document)
            except Exception as e:
                logger.warning(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {doc_file}, {e}")
        
        return documents
    
    async def list_by_status(
        self, 
        status: DocumentStatus, 
        project_id: Optional[str] = None
    ) -> List[Document]:
        """æ ¹æ®çŠ¶æ€åˆ—å‡ºæ–‡æ¡£"""
        documents = []
        
        for doc_file in self.base_path.glob("*.json"):
            try:
                with open(doc_file, 'r', encoding='utf-8') as f:
                    doc_data = json.load(f)
                
                if (doc_data.get('status') == status.value and
                    (project_id is None or doc_data.get('project_id') == project_id)):
                    document = await self.load(doc_data['id'])
                    if document:
                        documents.append(document)
            except Exception as e:
                logger.warning(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {doc_file}, {e}")
        
        return documents
    
    async def search(
        self, 
        query: str, 
        project_id: Optional[str] = None
    ) -> List[Document]:
        """æœç´¢æ–‡æ¡£"""
        documents = []
        query_lower = query.lower()
        
        for doc_file in self.base_path.glob("*.json"):
            try:
                with open(doc_file, 'r', encoding='utf-8') as f:
                    doc_data = json.load(f)
                
                if project_id and doc_data.get('project_id') != project_id:
                    continue
                
                # æœç´¢æ ‡é¢˜å’Œæè¿°
                metadata = doc_data.get('metadata', {})
                title = metadata.get('title', '').lower()
                description = metadata.get('description', '').lower()
                tags = metadata.get('tags', [])
                
                if (query_lower in title or 
                    query_lower in description or
                    any(query_lower in tag.lower() for tag in tags)):
                    document = await self.load(doc_data['id'])
                    if document:
                        documents.append(document)
            except Exception as e:
                logger.warning(f"æœç´¢æ–‡æ¡£å¤±è´¥: {doc_file}, {e}")
        
        return documents
    
    async def search_content(
        self, 
        query: str, 
        project_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£å†…å®¹"""
        results = []
        query_lower = query.lower()
        
        for content_file in self.base_path.glob("*_content.txt"):
            try:
                document_id = content_file.stem.replace('_content', '')
                
                # æ£€æŸ¥é¡¹ç›®ID
                if project_id:
                    doc_path = self._get_document_path(document_id)
                    if doc_path.exists():
                        with open(doc_path, 'r', encoding='utf-8') as f:
                            doc_data = json.load(f)
                        if doc_data.get('project_id') != project_id:
                            continue
                
                # æœç´¢å†…å®¹
                with open(content_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if query_lower in content.lower():
                    # æŸ¥æ‰¾åŒ¹é…çš„ä¸Šä¸‹æ–‡
                    lines = content.split('\n')
                    matches = []
                    
                    for i, line in enumerate(lines):
                        if query_lower in line.lower():
                            # è·å–ä¸Šä¸‹æ–‡ï¼ˆå‰åå„2è¡Œï¼‰
                            start = max(0, i - 2)
                            end = min(len(lines), i + 3)
                            context = '\n'.join(lines[start:end])
                            
                            matches.append({
                                "line_number": i + 1,
                                "line": line.strip(),
                                "context": context
                            })
                    
                    if matches:
                        results.append({
                            "document_id": document_id,
                            "matches": matches
                        })
            
            except Exception as e:
                logger.warning(f"æœç´¢å†…å®¹å¤±è´¥: {content_file}, {e}")
        
        return results
    
    async def get_recent_documents(
        self, 
        limit: int = 10, 
        project_id: Optional[str] = None
    ) -> List[Document]:
        """è·å–æœ€è¿‘ç¼–è¾‘çš„æ–‡æ¡£"""
        documents = []
        
        for doc_file in self.base_path.glob("*.json"):
            try:
                with open(doc_file, 'r', encoding='utf-8') as f:
                    doc_data = json.load(f)
                
                if project_id and doc_data.get('project_id') != project_id:
                    continue
                
                document = await self.load(doc_data['id'])
                if document:
                    documents.append(document)
            except Exception as e:
                logger.warning(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {doc_file}, {e}")
        
        # æŒ‰æ›´æ–°æ—¶é—´æ’åº
        documents.sort(
            key=lambda d: d.metadata.updated_at,
            reverse=True
        )
        
        return documents[:limit]
    
    async def update_content(self, document_id: str, content: str) -> bool:
        """æ›´æ–°æ–‡æ¡£å†…å®¹"""
        try:
            document = await self.load(document_id)
            if not document:
                return False
            
            document.content = content
            return await self.save(document)
            
        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            return False
    
    async def update_metadata(
        self, 
        document_id: str, 
        metadata: Dict[str, Any]
    ) -> bool:
        """æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®"""
        try:
            document = await self.load(document_id)
            if not document:
                return False
            
            # æ›´æ–°å…ƒæ•°æ®
            for key, value in metadata.items():
                if hasattr(document.metadata, key):
                    setattr(document.metadata, key, value)
            
            document.metadata.updated_at = datetime.now()
            return await self.save(document)
            
        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®å¤±è´¥: {e}")
            return False
    
    async def get_word_count(self, document_id: str) -> int:
        """è·å–æ–‡æ¡£å­—æ•°"""
        try:
            content_path = self._get_content_path(document_id)
            if not content_path.exists():
                return 0
            
            with open(content_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return len(content.split()) if content.strip() else 0
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£å­—æ•°å¤±è´¥: {e}")
            return 0
    
    async def get_statistics(self, document_id: str) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        try:
            document = await self.load(document_id)
            if not document:
                return {}
            
            return {
                "document_id": document_id,
                "title": document.title,
                "word_count": document.statistics.word_count,
                "character_count": document.statistics.character_count,
                "paragraph_count": document.statistics.paragraph_count,
                "sentence_count": document.statistics.sentence_count,
                "reading_time_minutes": document.statistics.reading_time_minutes,
                "last_edit_time": document.statistics.last_edit_time.isoformat() if document.statistics.last_edit_time else None,
                "edit_count": document.statistics.edit_count,
                "created_at": document.metadata.created_at.isoformat(),
                "updated_at": document.metadata.updated_at.isoformat(),
            }
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    # ç‰ˆæœ¬ç®¡ç†æ–¹æ³•ï¼ˆç®€å•å®ç°ï¼‰
    async def cleanup_old_versions(self, document_id: str, keep_count: int = 10) -> bool:
        """æ¸…ç†æ—§ç‰ˆæœ¬"""
        try:
            # è·å–æ–‡æ¡£çš„ç‰ˆæœ¬ç›®å½•
            doc_path = self._get_document_path(document_id)
            if not doc_path.exists():
                logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨ï¼Œæ— æ³•æ¸…ç†ç‰ˆæœ¬: {document_id}")
                return False

            # ç‰ˆæœ¬æ–‡ä»¶å­˜å‚¨åœ¨åŒç›®å½•ä¸‹ï¼Œä»¥ {document_id}_v{timestamp}.txt å‘½å
            doc_dir = doc_path.parent
            version_pattern = f"{document_id}_v*.txt"
            version_files = list(doc_dir.glob(version_pattern))

            if len(version_files) <= keep_count:
                logger.debug(f"ç‰ˆæœ¬æ•°é‡({len(version_files)})æœªè¶…è¿‡ä¿ç•™æ•°é‡({keep_count})ï¼Œæ— éœ€æ¸…ç†")
                return True

            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„ç‰ˆæœ¬
            version_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
            files_to_delete = version_files[keep_count:]

            deleted_count = 0
            for version_file in files_to_delete:
                try:
                    version_file.unlink()
                    deleted_count += 1
                    logger.debug(f"åˆ é™¤æ—§ç‰ˆæœ¬æ–‡ä»¶: {version_file.name}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤ç‰ˆæœ¬æ–‡ä»¶å¤±è´¥ {version_file}: {e}")

            logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§ç‰ˆæœ¬æ–‡ä»¶")
            return True

        except Exception as e:
            logger.error(f"æ¸…ç†æ—§ç‰ˆæœ¬å¤±è´¥: {e}")
            return False

    async def delete_version(self, document_id: str, version_id: str) -> bool:
        """åˆ é™¤æŒ‡å®šç‰ˆæœ¬"""
        try:
            # ç‰ˆæœ¬IDæ ¼å¼ä¸ºæ—¶é—´æˆ³ï¼Œç‰ˆæœ¬æ–‡ä»¶åä¸º {document_id}_v{version_id}.txt
            doc_path = self._get_document_path(document_id)
            if not doc_path.exists():
                logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨: {document_id}")
                return False

            doc_dir = doc_path.parent
            version_file = doc_dir / f"{document_id}_v{version_id}.txt"

            if not version_file.exists():
                logger.warning(f"ç‰ˆæœ¬æ–‡ä»¶ä¸å­˜åœ¨: {version_file}")
                return False

            version_file.unlink()
            logger.info(f"åˆ é™¤ç‰ˆæœ¬æˆåŠŸ: {document_id} ç‰ˆæœ¬ {version_id}")
            return True

        except Exception as e:
            logger.error(f"åˆ é™¤ç‰ˆæœ¬å¤±è´¥: {e}")
            return False

    async def create_version(self, document_id: str, content: str, description: str = "") -> Optional[str]:
        """åˆ›å»ºæ–‡æ¡£ç‰ˆæœ¬"""
        try:
            doc_path = self._get_document_path(document_id)
            if not doc_path.exists():
                logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨: {document_id}")
                return None

            # ç”Ÿæˆç‰ˆæœ¬IDï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
            version_id = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ç²¾ç¡®åˆ°æ¯«ç§’

            # åˆ›å»ºç‰ˆæœ¬æ–‡ä»¶
            doc_dir = doc_path.parent
            version_file = doc_dir / f"{document_id}_v{version_id}.txt"

            # ä¿å­˜ç‰ˆæœ¬å†…å®¹
            with open(version_file, 'w', encoding='utf-8') as f:
                f.write(content)

            # åˆ›å»ºç‰ˆæœ¬å…ƒæ•°æ®æ–‡ä»¶
            version_meta_file = doc_dir / f"{document_id}_v{version_id}.meta.json"
            version_meta = {
                "version_id": version_id,
                "document_id": document_id,
                "created_at": datetime.now().isoformat(),
                "description": description,
                "size": len(content)
            }

            with open(version_meta_file, 'w', encoding='utf-8') as f:
                json.dump(version_meta, f, indent=2, ensure_ascii=False)

            logger.info(f"åˆ›å»ºç‰ˆæœ¬æˆåŠŸ: {document_id} ç‰ˆæœ¬ {version_id}")

            # è‡ªåŠ¨æ¸…ç†æ—§ç‰ˆæœ¬ï¼ˆä¿ç•™æœ€è¿‘20ä¸ªç‰ˆæœ¬ï¼‰
            await self.cleanup_old_versions(document_id, 20)

            return version_id

        except Exception as e:
            logger.error(f"åˆ›å»ºç‰ˆæœ¬å¤±è´¥: {e}")
            return None

    async def get_version_diff(self, document_id: str, version1_id: str, version2_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ç‰ˆæœ¬å·®å¼‚"""
        try:
            doc_path = self._get_document_path(document_id)
            if not doc_path.exists():
                logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨: {document_id}")
                return None

            doc_dir = doc_path.parent

            # è·å–ä¸¤ä¸ªç‰ˆæœ¬çš„å†…å®¹
            version1_file = doc_dir / f"{document_id}_v{version1_id}.txt"
            version2_file = doc_dir / f"{document_id}_v{version2_id}.txt"

            if not version1_file.exists():
                logger.warning(f"ç‰ˆæœ¬1æ–‡ä»¶ä¸å­˜åœ¨: {version1_file}")
                return None

            if not version2_file.exists():
                logger.warning(f"ç‰ˆæœ¬2æ–‡ä»¶ä¸å­˜åœ¨: {version2_file}")
                return None

            # è¯»å–ç‰ˆæœ¬å†…å®¹
            with open(version1_file, 'r', encoding='utf-8') as f:
                content1 = f.read()

            with open(version2_file, 'r', encoding='utf-8') as f:
                content2 = f.read()

            # ç®€å•çš„å·®å¼‚åˆ†æ
            lines1 = content1.splitlines()
            lines2 = content2.splitlines()

            # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            diff_info = {
                "document_id": document_id,
                "version1_id": version1_id,
                "version2_id": version2_id,
                "version1_lines": len(lines1),
                "version2_lines": len(lines2),
                "version1_chars": len(content1),
                "version2_chars": len(content2),
                "lines_added": 0,
                "lines_removed": 0,
                "lines_modified": 0,
                "changes": []
            }

            # ç®€å•çš„é€è¡Œæ¯”è¾ƒ
            max_lines = max(len(lines1), len(lines2))
            for i in range(max_lines):
                line1 = lines1[i] if i < len(lines1) else None
                line2 = lines2[i] if i < len(lines2) else None

                if line1 is None:
                    # æ–°å¢è¡Œ
                    diff_info["lines_added"] += 1
                    diff_info["changes"].append({
                        "type": "added",
                        "line_number": i + 1,
                        "content": line2
                    })
                elif line2 is None:
                    # åˆ é™¤è¡Œ
                    diff_info["lines_removed"] += 1
                    diff_info["changes"].append({
                        "type": "removed",
                        "line_number": i + 1,
                        "content": line1
                    })
                elif line1 != line2:
                    # ä¿®æ”¹è¡Œ
                    diff_info["lines_modified"] += 1
                    diff_info["changes"].append({
                        "type": "modified",
                        "line_number": i + 1,
                        "old_content": line1,
                        "new_content": line2
                    })

            logger.info(f"ç‰ˆæœ¬å·®å¼‚åˆ†æå®Œæˆ: {document_id} {version1_id} vs {version2_id}")
            return diff_info

        except Exception as e:
            logger.error(f"è·å–ç‰ˆæœ¬å·®å¼‚å¤±è´¥: {e}")
            return None

    async def restore_version(self, document_id: str, version_id: str) -> bool:
        """æ¢å¤åˆ°æŒ‡å®šç‰ˆæœ¬"""
        try:
            doc_path = self._get_document_path(document_id)
            if not doc_path.exists():
                logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨: {document_id}")
                return False

            doc_dir = doc_path.parent
            version_file = doc_dir / f"{document_id}_v{version_id}.txt"

            if not version_file.exists():
                logger.warning(f"ç‰ˆæœ¬æ–‡ä»¶ä¸å­˜åœ¨: {version_file}")
                return False

            # è¯»å–ç‰ˆæœ¬å†…å®¹
            with open(version_file, 'r', encoding='utf-8') as f:
                version_content = f.read()

            # è·å–å½“å‰æ–‡æ¡£
            document = await self.get_by_id(document_id)
            if not document:
                logger.warning(f"æ— æ³•è·å–æ–‡æ¡£: {document_id}")
                return False

            # åœ¨æ¢å¤å‰åˆ›å»ºå½“å‰ç‰ˆæœ¬çš„å¤‡ä»½
            current_content_path = doc_dir / f"{document_id}_content.txt"
            if current_content_path.exists():
                with open(current_content_path, 'r', encoding='utf-8') as f:
                    current_content = f.read()

                # åˆ›å»ºæ¢å¤å‰çš„å¤‡ä»½
                backup_version_id = await self.create_version(
                    document_id,
                    current_content,
                    f"æ¢å¤å‰å¤‡ä»½ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                )
                if backup_version_id:
                    logger.info(f"å·²åˆ›å»ºæ¢å¤å‰å¤‡ä»½: {backup_version_id}")

            # æ›´æ–°æ–‡æ¡£å†…å®¹
            document.content = version_content
            document.updated_at = datetime.now()

            # ä¿å­˜æ–‡æ¡£
            success = await self.save(document)

            if success:
                logger.info(f"ç‰ˆæœ¬æ¢å¤æˆåŠŸ: {document_id} -> ç‰ˆæœ¬ {version_id}")
                return True
            else:
                logger.error(f"ç‰ˆæœ¬æ¢å¤å¤±è´¥: ä¿å­˜æ–‡æ¡£æ—¶å‡ºé”™")
                return False

        except Exception as e:
            logger.error(f"æ¢å¤ç‰ˆæœ¬å¤±è´¥: {e}")
            return False

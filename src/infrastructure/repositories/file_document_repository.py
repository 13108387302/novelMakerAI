#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ä»¶ç³»ç»Ÿæ–‡æ¡£ä»“å‚¨å®ç°

åŸºäºæ–‡ä»¶ç³»ç»Ÿçš„æ–‡æ¡£æ•°æ®æŒä¹…åŒ–å®ç°
"""

import json
import re
from pathlib import Path
from typing import List, Optional, Dict, Any, AsyncGenerator
from datetime import datetime
import asyncio

from src.domain.entities.document import Document, DocumentType, DocumentStatus, create_document
from src.domain.repositories.document_repository import IDocumentRepository
from src.shared.utils.logger import get_logger
from src.shared.utils.unified_performance import get_performance_manager, performance_monitor
from src.shared.utils.unified_error_handler import get_error_handler, ErrorCategory, ErrorSeverity
from src.shared.utils.file_operations import get_file_operations
from src.shared.constants import (
    ENCODING_FORMATS, CACHE_EXPIRE_SECONDS, VERSION_KEEP_COUNT
)

logger = get_logger(__name__)

# æ–‡æ¡£ä»“å‚¨å¸¸é‡
DEFAULT_DOCUMENTS_DIR = ".novel_editor/documents"
DOCUMENT_METADATA_EXT = ".json"
DOCUMENT_CONTENT_SUFFIX = "_content.txt"
TEMP_FILE_EXT = ".tmp"
VERSION_FILE_PREFIX = "_v"
VERSION_META_SUFFIX = ".meta.json"
DEFAULT_ENCODING = ENCODING_FORMATS['utf8']
FALLBACK_ENCODING = ENCODING_FORMATS['gbk']
CACHE_PREFIX = "doc_repo"
SHORT_CACHE_TTL = 60  # 1åˆ†é’Ÿ
LONG_CACHE_TTL = CACHE_EXPIRE_SECONDS  # 5åˆ†é’Ÿ
DEFAULT_VERSION_KEEP_COUNT = VERSION_KEEP_COUNT
# æ–‡æ¡£ç±»å‹åˆ°å­ç›®å½•çš„æ˜ å°„ï¼ˆç›¸å¯¹äº base_pathï¼‰
DOC_TYPE_DIRS = {
    DocumentType.CHAPTER: "chapters",
    DocumentType.CHARACTER: "characters",
    DocumentType.SETTING: "settings",
    DocumentType.OUTLINE: "outlines",
    DocumentType.NOTE: "notes",
    DocumentType.RESEARCH: "research",
    DocumentType.TIMELINE: "timeline",
    DocumentType.WORLDBUILDING: "worldbuilding",
}

DEFAULT_CHUNK_SIZE = 8192
DEFAULT_LINE_COUNT = 1000
CONTEXT_LINES = 2  # æœç´¢ä¸Šä¸‹æ–‡è¡Œæ•°
ASYNC_SLEEP_MS = 0.001  # å¼‚æ­¥ç¡çœ æ—¶é—´


class FileDocumentRepository(IDocumentRepository):
    """
    æ–‡ä»¶ç³»ç»Ÿæ–‡æ¡£ä»“å‚¨å®ç°

    åŸºäºæ–‡ä»¶ç³»ç»Ÿçš„æ–‡æ¡£æ•°æ®æŒä¹…åŒ–å®ç°ï¼Œä½¿ç”¨JSONæ ¼å¼å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®ï¼Œ
    ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶å­˜å‚¨æ–‡æ¡£å†…å®¹ã€‚

    å®ç°æ–¹å¼ï¼š
    - ä½¿ç”¨JSONæ–‡ä»¶å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
    - ä½¿ç”¨ç‹¬ç«‹çš„æ–‡æœ¬æ–‡ä»¶å­˜å‚¨æ–‡æ¡£å†…å®¹
    - æ”¯æŒè·¨é¡¹ç›®çš„æ–‡æ¡£æŸ¥æ‰¾
    - æä¾›å®Œæ•´çš„CRUDæ“ä½œ
    - åŒ…å«æ–‡æ¡£å†…å®¹çš„æœç´¢åŠŸèƒ½

    Attributes:
        base_path: æ–‡æ¡£å­˜å‚¨çš„åŸºç¡€è·¯å¾„
    """

    def __init__(self, base_path: Path):
        """
        åˆå§‹åŒ–æ–‡ä»¶ç³»ç»Ÿæ–‡æ¡£ä»“å‚¨

        Args:
            base_path: æ–‡æ¡£å­˜å‚¨çš„åŸºç¡€è·¯å¾„ï¼ˆå¿…é¡»æä¾›ï¼Œé€šå¸¸ä¸ºé¡¹ç›®å†…è·¯å¾„ï¼‰
        """
        self.base_path = base_path
        self.base_path.mkdir(parents=True, exist_ok=True)

        # ä½¿ç”¨ç»Ÿä¸€çš„æ€§èƒ½ç®¡ç†å™¨
        self.performance_manager = get_performance_manager()
        self.error_handler = get_error_handler()

        # ç»Ÿä¸€æ–‡ä»¶æ“ä½œå·¥å…·
        self.file_ops = get_file_operations("document_repo")

        # ç¼“å­˜é”®å‰ç¼€
        self._cache_prefix = CACHE_PREFIX

    def _get_doc_dir_for_type(self, doc_type: Optional[DocumentType]) -> Path:
        """æ ¹æ®æ–‡æ¡£ç±»å‹è·å–ç›®å½•ï¼ˆé»˜è®¤å›é€€ base_pathï¼‰"""
        try:
            if not doc_type:
                return self.base_path
            sub = DOC_TYPE_DIRS.get(doc_type)
            if not sub:
                return self.base_path
            path = self.base_path / sub
            path.mkdir(parents=True, exist_ok=True)
            return path
        except Exception:
            return self.base_path

    def _get_document_path(self, document_id: str, doc_type: Optional[DocumentType] = None) -> Path:
        """è·å–æ–‡æ¡£å…ƒæ•°æ®è·¯å¾„ï¼ˆæŒ‰ç±»å‹å­ç›®å½•è·¯ç”±ï¼‰"""
        base = self._get_doc_dir_for_type(doc_type)
        return base / f"{document_id}{DOCUMENT_METADATA_EXT}"

    def _get_content_path(self, document_id: str, doc_type: Optional[DocumentType] = None) -> Path:
        """è·å–æ–‡æ¡£å†…å®¹è·¯å¾„ï¼ˆæŒ‰ç±»å‹å­ç›®å½•è·¯ç”±ï¼‰"""
        base = self._get_doc_dir_for_type(doc_type)
        return base / f"{document_id}{DOCUMENT_CONTENT_SUFFIX}"

    async def _read_text_file_safe(self, file_path: Path) -> str:
        """å®‰å…¨è¯»å–æ–‡æœ¬æ–‡ä»¶ï¼Œæ”¯æŒç¼–ç å›é€€ï¼ˆå§”æ‰˜ç»Ÿä¸€å®ç°ï¼‰"""
        try:
            from src.shared.utils.file_operations import get_file_operations
            ops = get_file_operations()
            content = await ops.load_text_safe(file_path)
            return content or ""
        except Exception as e:
            logger.error(f"è¯»å–æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {file_path}, {e}")
            return ""

    def _build_document_from_data(self, doc_data: dict, content: str = "") -> Optional[Document]:
        """ä» JSON æ•°æ®æ„å»º Document å¯¹è±¡çš„ç»Ÿä¸€æ–¹æ³•"""
        try:
            # éªŒè¯å¿…è¦å­—æ®µ
            if not isinstance(doc_data, dict):
                logger.error("æ–‡æ¡£æ•°æ®æ ¼å¼æ— æ•ˆï¼šä¸æ˜¯å­—å…¸ç±»å‹")
                return None

            if 'id' not in doc_data or 'metadata' not in doc_data:
                logger.error("æ–‡æ¡£æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µï¼šid æˆ– metadata")
                return None

            metadata = doc_data['metadata']
            if 'title' not in metadata:
                logger.error("æ–‡æ¡£å…ƒæ•°æ®ç¼ºå°‘æ ‡é¢˜å­—æ®µ")
                return None

            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            document_type = DocumentType(doc_data.get('type', 'chapter'))
            document = create_document(
                document_type=document_type,
                title=metadata['title'],
                document_id=doc_data['id'],
                content=content,
                status=DocumentStatus(doc_data.get('status', 'draft')),
                project_id=doc_data.get('project_id')
            )

            # æ¢å¤å…¶ä»–å±æ€§
            document.metadata.description = metadata.get('description', '')
            document.metadata.tags = set(metadata.get('tags', []))
            document.metadata.author = metadata.get('author', '')

            # æ¢å¤æ—¶é—´æˆ³
            if metadata.get('created_at'):
                try:
                    document.metadata.created_at = datetime.fromisoformat(metadata['created_at'])
                except ValueError as e:
                    logger.warning(f"æ— æ•ˆçš„åˆ›å»ºæ—¶é—´æ ¼å¼: {metadata['created_at']}, {e}")

            if metadata.get('updated_at'):
                try:
                    document.metadata.updated_at = datetime.fromisoformat(metadata['updated_at'])
                except ValueError as e:
                    logger.warning(f"æ— æ•ˆçš„æ›´æ–°æ—¶é—´æ ¼å¼: {metadata['updated_at']}, {e}")

            return document

        except Exception as e:
            logger.error(f"æ„å»ºæ–‡æ¡£å¯¹è±¡å¤±è´¥: {e}")
            return None

    async def _find_document_in_projects(self, document_id: str) -> tuple[Optional[Path], Optional[Path]]:
        """åœ¨æ‰€æœ‰é¡¹ç›®ç›®å½•ä¸­æŸ¥æ‰¾æ–‡æ¡£ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""
        try:
            # æ£€æŸ¥ç»Ÿä¸€ç¼“å­˜
            cache_key = f"{self._cache_prefix}:doc_paths:{document_id}"
            cache_result = self.performance_manager.cache_get(cache_key)

            if cache_result.success:
                cached_paths = cache_result.data
                if cached_paths[0] and cached_paths[0].exists():
                    logger.debug(f"âš¡ ä»ç¼“å­˜ä¸­æ‰¾åˆ°æ–‡æ¡£: {cached_paths[0]}")
                    return cached_paths
                else:
                    # ç¼“å­˜çš„è·¯å¾„ä¸å­˜åœ¨ï¼Œç§»é™¤ç¼“å­˜
                    self.performance_manager.cache_delete(cache_key)

            # åœ¨æ‰€æœ‰ç±»å‹å­ç›®å½•ä¸­æŸ¥æ‰¾ï¼ˆä¼˜å…ˆç¼“å­˜ï¼‰
            for sub in set(DOC_TYPE_DIRS.values()) | {""}:
                base = self.base_path / sub if sub else self.base_path
                doc_path = base / f"{document_id}.json"
                content_path = base / f"{document_id}_content.txt"
                if doc_path.exists():
                    logger.debug(f"ğŸ” åœ¨é¡¹ç›®ç›®å½•ä¸­æ‰¾åˆ°æ–‡æ¡£: {doc_path}")
                    cache_key = f"{self._cache_prefix}:doc_paths:{document_id}"
                    self.performance_manager.cache_set(cache_key, (doc_path, content_path), ttl=LONG_CACHE_TTL)
                    return doc_path, content_path

            return None, None

        except Exception as e:
            logger.error(f"åœ¨é¡¹ç›®ä¸­æŸ¥æ‰¾æ–‡æ¡£å¤±è´¥: {e}")
            return None, None

    async def _get_document_save_path(self, document: Document) -> Path:
        """è·å–æ–‡æ¡£ä¿å­˜è·¯å¾„ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥ä½¿ç”¨base_pathï¼‰"""
        logger.debug(f"è·å–æ–‡æ¡£ä¿å­˜è·¯å¾„ï¼Œé¡¹ç›®ID: {document.project_id}")

        # ç›´æ¥ä½¿ç”¨base_pathï¼Œå› ä¸ºå®ƒå·²ç»æ˜¯æ­£ç¡®çš„é¡¹ç›®æ–‡æ¡£ç›®å½•
        # base_path åœ¨æœåŠ¡æ³¨å†Œæ—¶è®¾ç½®ä¸º project_paths.documents_dir
        logger.debug(f"ä½¿ç”¨æ–‡æ¡£ä»“å‚¨base_path: {self.base_path}")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.base_path.mkdir(parents=True, exist_ok=True)

        return self.base_path

    async def save(self, document: Document) -> bool:
        """ä¿å­˜æ–‡æ¡£ï¼ˆå¸¦ç¼“å­˜æ¸…ç†ï¼‰"""
        doc_temp_file = None
        content_temp_file = None
        try:
            # ç¡®å®šä¿å­˜è·¯å¾„
            save_path = await self._get_document_save_path(document)
            logger.info(f"ğŸ’¾ æ–‡æ¡£ä¿å­˜è·¯å¾„: {save_path}")
            logger.info(f"ğŸ“‹ æ–‡æ¡£é¡¹ç›®ID: {document.project_id}")

            # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®ï¼ˆæŒ‰ç±»å‹ç›®å½•ï¼‰
            doc_path = self._get_document_path(document.id, document.type)
            doc_path.parent.mkdir(parents=True, exist_ok=True)
            doc_temp_file = doc_path.with_suffix('.tmp')
            doc_data = document.to_dict()

            # éªŒè¯é¡¹ç›®IDæ˜¯å¦æ­£ç¡®ä¿å­˜
            if doc_data.get('project_id') != document.project_id:
                logger.error(f"âŒ æ–‡æ¡£æ•°æ®ä¸­çš„é¡¹ç›®IDä¸åŒ¹é…: æœŸæœ› {document.project_id}, å®é™… {doc_data.get('project_id')}")
            else:
                logger.debug(f"âœ… æ–‡æ¡£é¡¹ç›®IDéªŒè¯é€šè¿‡: {document.project_id}")

            # åˆ†ç¦»å†…å®¹å’Œå…ƒæ•°æ®
            content = doc_data.pop('content', '')

            # ä½¿ç”¨ç»Ÿä¸€æ–‡ä»¶æ“ä½œä¿å­˜å…ƒæ•°æ®
            cache_key = f"metadata:{document.id}"
            metadata_success = await self.file_ops.save_json_atomic(
                file_path=doc_path,
                data=doc_data,
                create_backup=True,
                cache_key=cache_key,
                cache_ttl=3600
            )

            if not metadata_success:
                logger.error(f"âŒ ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®å¤±è´¥: {document.id}")
                return False

            # ä½¿ç”¨ç»Ÿä¸€æ–‡ä»¶æ“ä½œä¿å­˜å†…å®¹
            content_path = self._get_content_path(document.id, document.type)
            content_success = await self.file_ops.save_text_atomic(
                file_path=content_path,
                content=content or '',
                create_backup=True
            )

            if not content_success:
                logger.error(f"âŒ ä¿å­˜æ–‡æ¡£å†…å®¹å¤±è´¥: {document.id}")
                return False

            # åˆ›å»ºç‰ˆæœ¬å¤‡ä»½ï¼ˆå¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼‰
            if content and len(content.strip()) > 0:
                try:
                    # ç›´æ¥ä¼ é€’æ–‡æ¡£è·¯å¾„ï¼Œé¿å…æŸ¥æ‰¾é—®é¢˜
                    version_id = await self._create_version_with_path(
                        document.id,
                        content,
                        doc_path,
                        f"è‡ªåŠ¨ä¿å­˜ç‰ˆæœ¬ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                    )
                    if version_id:
                        logger.debug(f"åˆ›å»ºç‰ˆæœ¬å¤‡ä»½: {document.id} -> {version_id}")
                except Exception as e:
                    logger.warning(f"åˆ›å»ºç‰ˆæœ¬å¤‡ä»½å¤±è´¥: {e}")
                    # ç‰ˆæœ¬åˆ›å»ºå¤±è´¥ä¸å½±å“æ–‡æ¡£ä¿å­˜

            # æ¸…ç†ç›¸å…³ç¼“å­˜
            self._clear_project_cache(document.project_id)

            logger.info(f"æ–‡æ¡£ä¿å­˜æˆåŠŸ: {document.title} ({document.id})")
            return True

        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if doc_temp_file and doc_temp_file.exists():
                try:
                    doc_temp_file.unlink()
                except Exception:
                    pass
            if content_temp_file and content_temp_file.exists():
                try:
                    content_temp_file.unlink()
                except Exception:
                    pass
            logger.error(f"ä¿å­˜æ–‡æ¡£å¤±è´¥: {e}")
            return False

    @performance_monitor("æ–‡æ¡£åŠ è½½")
    async def load(self, document_id: str) -> Optional[Document]:
        """æ ¹æ®IDåŠ è½½æ–‡æ¡£ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # é¦–å…ˆå°è¯•ä»é»˜è®¤è·¯å¾„åŠ è½½
            # åœ¨æ‰€æœ‰ç±»å‹å­ç›®å½•ä¸­å°è¯•å®šä½
            doc_path = None
            content_path = None
            for sub in set(DOC_TYPE_DIRS.values()) | {""}:
                base = self.base_path / sub if sub else self.base_path
                test_doc = base / f"{document_id}.json"
                test_content = base / f"{document_id}_content.txt"
                if test_doc.exists():
                    doc_path, content_path = test_doc, test_content
                    break

            # å¦‚æœæœªæ‰¾åˆ°ï¼Œå°è¯•åœ¨é¡¹ç›®ä¸­æŸ¥æ‰¾
            if not doc_path:
                doc_path, content_path = await self._find_document_in_projects(document_id)
                if not doc_path or not doc_path.exists():
                    return None

            # ä½¿ç”¨ç»Ÿä¸€æ–‡ä»¶æ“ä½œåŠ è½½å…ƒæ•°æ®
            cache_key = f"metadata:{document_id}"
            doc_data = await self.file_ops.load_json_cached(
                file_path=doc_path,
                cache_key=cache_key,
                cache_ttl=3600
            )

            if not doc_data:
                return None

            # éªŒè¯æ•°æ®æ ¼å¼
            if not isinstance(doc_data, dict):
                logger.error(f"æ–‡æ¡£å…ƒæ•°æ®æ ¼å¼æ— æ•ˆ: {doc_path}")
                return None

            # ä½¿ç”¨ç»Ÿä¸€æ–‡ä»¶æ“ä½œåŠ è½½å†…å®¹
            content = ""
            if content_path and content_path.exists():
                content = await self.file_ops.load_text(content_path) or ""

            # ä½¿ç”¨ç»Ÿä¸€çš„æ„å»ºæ–¹æ³•
            document = self._build_document_from_data(doc_data, content)
            if not document:
                return None

            logger.info(f"âš¡ æ–‡æ¡£åŠ è½½æˆåŠŸ: {document.title} ({document.id})")
            return document

        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            return None

    async def delete(self, document_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        try:
            # æŒ‰æ‰€æœ‰ç±»å‹å­ç›®å½•å°è¯•åˆ é™¤
            deleted = False
            for sub in set(DOC_TYPE_DIRS.values()) | {""}:
                base = self.base_path / sub if sub else self.base_path
                doc_path = base / f"{document_id}.json"
                content_path = base / f"{document_id}_content.txt"
                if doc_path.exists():
                    doc_path.unlink()
                    deleted = True
                if content_path.exists():
                    content_path.unlink()
                    deleted = True
            if not deleted:
                # å…œåº•ï¼šé¡¹ç›®èŒƒå›´æŸ¥æ‰¾
                doc_path, content_path = await self._find_document_in_projects(document_id)
                if doc_path and doc_path.exists():
                    doc_path.unlink()
                if content_path and content_path.exists():
                    content_path.unlink()

            logger.info(f"æ–‡æ¡£åˆ é™¤æˆåŠŸ: {document_id}")
            return True

        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False

    async def exists(self, document_id: str) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨"""
        # é¦–å…ˆæ£€æŸ¥é»˜è®¤è·¯å¾„
        # åœ¨æ‰€æœ‰ç±»å‹å­ç›®å½•ä¸­æŸ¥æ‰¾
        for sub in set(DOC_TYPE_DIRS.values()) | {""}:
            base = self.base_path / sub if sub else self.base_path
            if (base / f"{document_id}.json").exists():
                return True

        # å¦‚æœæœªæ‰¾åˆ°ï¼Œæ£€æŸ¥é¡¹ç›®ç›®å½•
        found_paths = await self._find_document_in_projects(document_id)
        return found_paths is not None

    async def list_by_project(self, project_id: str) -> List[Document]:
        """åˆ—å‡ºé¡¹ç›®ä¸­çš„æ‰€æœ‰æ–‡æ¡£ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            import time
            start_time = time.time()

            logger.info(f"ğŸ“‹ å¼€å§‹è·å–é¡¹ç›®æ–‡æ¡£åˆ—è¡¨: {project_id}")

            # ä½¿ç”¨ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
            cache_key = f"{self._cache_prefix}:project_docs:{project_id}"
            cache_result = self.performance_manager.cache_get(cache_key)
            if cache_result.success:
                cached_documents = cache_result.data
                logger.info(f"âš¡ ä»ç¼“å­˜è·å–é¡¹ç›®æ–‡æ¡£: {len(cached_documents)} ä¸ª")
                return cached_documents

            documents = []
            found_doc_ids = set()

            # ä¼˜åŒ–çš„æ–‡æ¡£æŸ¥æ‰¾ç­–ç•¥
            search_paths = await self._get_project_document_paths(project_id)

            for search_path in search_paths:
                if not search_path.exists():
                    continue

                logger.debug(f"ğŸ” æœç´¢è·¯å¾„: {search_path}")

                # æ‰¹é‡è¯»å–æ–‡æ¡£å…ƒæ•°æ®ï¼Œé¿å…é€ä¸ªåŠ è½½å®Œæ•´æ–‡æ¡£
                # æ’é™¤ç‰ˆæœ¬å…ƒæ•°æ®æ–‡ä»¶å’Œå…¶ä»–éæ–‡æ¡£æ–‡ä»¶
                all_json_files = list(search_path.glob("*.json"))
                doc_files = [
                    f for f in all_json_files
                    if not f.name.endswith('.meta.json') and '_v' not in f.stem
                ]
                logger.debug(f"ğŸ“„ æ‰¾åˆ° {len(all_json_files)} ä¸ªJSONæ–‡ä»¶ï¼Œå…¶ä¸­ {len(doc_files)} ä¸ªæ˜¯æ–‡æ¡£æ–‡ä»¶")

                for doc_file in doc_files:
                    try:
                        # åªè¯»å–å…ƒæ•°æ®ï¼Œä¸åŠ è½½å†…å®¹
                        doc_data = await self.file_ops.load_json_cached(
                            file_path=doc_file,
                            cache_key=f"{self._cache_prefix}:meta:{doc_file.stem}",
                            cache_ttl=300
                        )

                        # éªŒè¯æ–‡æ¡£æ•°æ®çš„åŸºæœ¬ç»“æ„
                        if not self._validate_document_data(doc_data, project_id):
                            # å°è¯•ä¿®å¤ç¼ºå°‘IDçš„æ–‡æ¡£æ•°æ®
                            if await self._try_fix_document_data(doc_data, doc_file, project_id):
                                logger.info(f"æˆåŠŸä¿®å¤æ–‡æ¡£æ•°æ®: {doc_file.name}")
                            else:
                                logger.warning(f"è·³è¿‡æ— æ•ˆçš„æ–‡æ¡£æ–‡ä»¶: {doc_file.name}")
                                continue

                        if doc_data.get('id') not in found_doc_ids:
                            # åˆ›å»ºè½»é‡çº§æ–‡æ¡£å¯¹è±¡ï¼ˆä¸åŠ è½½å†…å®¹ï¼‰
                            document = await self._create_lightweight_document(doc_data)
                            if document:
                                documents.append(document)
                                found_doc_ids.add(document.id)
                                logger.debug(f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£: {document.title}")

                    except json.JSONDecodeError as e:
                        logger.warning(f"JSONæ ¼å¼é”™è¯¯: {doc_file}, {e}")
                    except Exception as e:
                        logger.warning(f"è¯»å–æ–‡æ¡£å…ƒæ•°æ®å¤±è´¥: {doc_file}, {e}")

            # ç¼“å­˜ç»“æœåˆ°ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
            cache_key = f"{self._cache_prefix}:project_docs:{project_id}"
            self.performance_manager.cache_set(cache_key, documents, ttl=60)  # 1åˆ†é’Ÿç¼“å­˜

            load_time = time.time() - start_time
            logger.info(f"âš¡ é¡¹ç›®æ–‡æ¡£åˆ—è¡¨è·å–å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£, è€—æ—¶: {load_time:.3f}s")

            return documents

        except Exception as e:
            logger.error(f"âŒ è·å–é¡¹ç›®æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def _get_project_document_paths(self, project_id: str) -> List[Path]:
        """è·å–é¡¹ç›®æ–‡æ¡£çš„æœç´¢è·¯å¾„ï¼ˆåŒ…å«ç±»å‹å­ç›®å½•ï¼‰"""
        try:
            # åŸºäº base_path æ„å»ºï¼šæ ¹ç›®å½• + å„ç±»å‹å­ç›®å½•
            paths = [self.base_path]
            for sub in set(DOC_TYPE_DIRS.values()):
                paths.append(self.base_path / sub)

            logger.debug(f"é¡¹ç›® {project_id} çš„æ–‡æ¡£æœç´¢è·¯å¾„: {[str(p) for p in paths]}")
            return paths

        except Exception as e:
            logger.error(f"è·å–é¡¹ç›®æ–‡æ¡£è·¯å¾„å¤±è´¥: {e}")
            return [self.base_path]

    async def _get_project_root_path(self, project_id: str) -> Optional[Path]:
        """è·å–é¡¹ç›®æ ¹è·¯å¾„"""
        try:
            # æ–¹æ³•1ï¼šå°è¯•ä»ä¾èµ–æ³¨å…¥å®¹å™¨è·å–å½“å‰é¡¹ç›®è·¯å¾„
            try:
                from src.shared.ioc.container import get_global_container
                from src.shared.project_context import ProjectPaths

                container = get_global_container()
                if container:
                    project_paths = container.try_get(ProjectPaths)
                    if project_paths:
                        logger.debug(f"ä»å®¹å™¨è·å–é¡¹ç›®æ ¹è·¯å¾„: {project_paths.root}")
                        return project_paths.root
            except Exception as e:
                logger.debug(f"ä»å®¹å™¨è·å–é¡¹ç›®è·¯å¾„å¤±è´¥: {e}")

            # æ–¹æ³•2ï¼šå°è¯•ä»é¡¹ç›®ä»“åº“è·å–é¡¹ç›®ä¿¡æ¯
            try:
                from src.infrastructure.repositories.file_project_repository import FileProjectRepository

                # ä½¿ç”¨å½“å‰æ–‡æ¡£ä»“å‚¨çš„base_pathçš„çˆ¶ç›®å½•ä½œä¸ºé¡¹ç›®ä»“å‚¨çš„base_path
                project_base_path = self.base_path.parent.parent / ".novel_editor" / "data"
                project_repo = FileProjectRepository(project_base_path)

                # å°è¯•åŠ è½½é¡¹ç›®
                project = await project_repo.get_by_id(project_id)
                if project and hasattr(project, 'root_path') and project.root_path:
                    logger.debug(f"ä»é¡¹ç›®ä»“å‚¨è·å–æ ¹è·¯å¾„: {project.root_path}")
                    return Path(project.root_path)
            except Exception as e:
                logger.debug(f"ä»é¡¹ç›®ä»“å‚¨è·å–é¡¹ç›®å¤±è´¥: {e}")

            # æ–¹æ³•3ï¼šåŸºäºæ–‡æ¡£ä»“å‚¨è·¯å¾„æ¨æ–­é¡¹ç›®æ ¹è·¯å¾„
            # å¦‚æœbase_pathæ˜¯ /project_root/content/documentsï¼Œåˆ™é¡¹ç›®æ ¹è·¯å¾„æ˜¯ /project_root
            if "content" in self.base_path.parts and "documents" in self.base_path.parts:
                # æ‰¾åˆ°contentç›®å½•çš„ä½ç½®
                parts = self.base_path.parts
                content_index = parts.index("content")
                if content_index > 0:
                    project_root = Path(*parts[:content_index])
                    logger.debug(f"åŸºäºè·¯å¾„æ¨æ–­é¡¹ç›®æ ¹è·¯å¾„: {project_root}")
                    return project_root

            return None

        except Exception as e:
            logger.debug(f"è·å–é¡¹ç›®æ ¹è·¯å¾„å¤±è´¥: {e}")
            return None

    def _validate_document_data(self, doc_data: dict, project_id: str) -> bool:
        """éªŒè¯æ–‡æ¡£æ•°æ®çš„åŸºæœ¬ç»“æ„"""
        try:
            # æ£€æŸ¥åŸºæœ¬å­—æ®µ
            if not isinstance(doc_data, dict):
                logger.debug("æ–‡æ¡£æ•°æ®ä¸æ˜¯å­—å…¸ç±»å‹")
                return False

            # æ£€æŸ¥ID - å¦‚æœç¼ºå°‘ï¼Œå°è¯•ä»æ–‡ä»¶åæ¨æ–­
            if not doc_data.get('id'):
                logger.debug("æ–‡æ¡£æ•°æ®ç¼ºå°‘IDå­—æ®µï¼Œå¯èƒ½æ˜¯æ—§ç‰ˆæœ¬æ–‡ä»¶")
                return False

            # æ£€æŸ¥é¡¹ç›®IDåŒ¹é…
            doc_project_id = doc_data.get('project_id')
            if doc_project_id != project_id:
                logger.debug(f"é¡¹ç›®IDä¸åŒ¹é…: æœŸæœ› {project_id}, å®é™… {doc_project_id}")
                # ä¸¥æ ¼åŒ¹é…é¡¹ç›®IDï¼Œé¿å…åŠ è½½å…¶ä»–é¡¹ç›®çš„æ–‡æ¡£
                return False

            # æ£€æŸ¥æ–‡æ¡£ç±»å‹
            doc_type = doc_data.get('type') or doc_data.get('document_type')
            if not doc_type:
                logger.debug("æ–‡æ¡£æ•°æ®ç¼ºå°‘ç±»å‹å­—æ®µï¼Œä½¿ç”¨é»˜è®¤ç±»å‹")
                # ä¸ç›´æ¥è¿”å›Falseï¼Œè€Œæ˜¯åœ¨åç»­å¤„ç†ä¸­è®¾ç½®é»˜è®¤ç±»å‹

            return True

        except Exception as e:
            logger.debug(f"éªŒè¯æ–‡æ¡£æ•°æ®å¤±è´¥: {e}")
            return False

    async def _try_fix_document_data(self, doc_data: dict, doc_file: Path, project_id: str) -> bool:
        """å°è¯•ä¿®å¤ç¼ºå°‘å­—æ®µçš„æ–‡æ¡£æ•°æ®"""
        try:
            fixed = False

            # ä¿®å¤ç¼ºå°‘çš„IDå­—æ®µ
            if not doc_data.get('id'):
                # ä»æ–‡ä»¶åæ¨æ–­ID
                file_stem = doc_file.stem
                if file_stem and file_stem != 'document':
                    doc_data['id'] = file_stem
                    fixed = True
                    logger.debug(f"ä»æ–‡ä»¶åæ¨æ–­æ–‡æ¡£ID: {file_stem}")
                else:
                    # ç”Ÿæˆæ–°çš„ID
                    import uuid
                    doc_data['id'] = str(uuid.uuid4())
                    fixed = True
                    logger.debug(f"ç”Ÿæˆæ–°çš„æ–‡æ¡£ID: {doc_data['id']}")

            # ä¿®å¤ç¼ºå°‘çš„é¡¹ç›®ID
            if not doc_data.get('project_id'):
                doc_data['project_id'] = project_id
                fixed = True
                logger.debug(f"è®¾ç½®æ–‡æ¡£é¡¹ç›®ID: {project_id}")

            # ä¿®å¤ç¼ºå°‘çš„æ–‡æ¡£ç±»å‹
            if not (doc_data.get('type') or doc_data.get('document_type')):
                doc_data['type'] = 'chapter'  # é»˜è®¤ç±»å‹
                fixed = True
                logger.debug("è®¾ç½®é»˜è®¤æ–‡æ¡£ç±»å‹: chapter")

            # ä¿®å¤ç¼ºå°‘çš„å…ƒæ•°æ®
            if not doc_data.get('metadata'):
                doc_data['metadata'] = {
                    'title': doc_data.get('title', 'æœªå‘½åæ–‡æ¡£'),
                    'description': '',
                    'tags': [],
                    'author': '',
                    'created_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat()
                }
                fixed = True
                logger.debug("æ·»åŠ é»˜è®¤å…ƒæ•°æ®")

            # å¦‚æœè¿›è¡Œäº†ä¿®å¤ï¼Œä¿å­˜ä¿®å¤åçš„æ–‡ä»¶
            if fixed:
                try:
                    await self.file_ops.save_json_atomic(
                        file_path=doc_file,
                        data=doc_data,
                        create_backup=True,
                        cache_key=f"{self._cache_prefix}:meta:{doc_file.stem}",
                        cache_ttl=300
                    )
                    logger.info(f"å·²ä¿å­˜ä¿®å¤åçš„æ–‡æ¡£æ•°æ®: {doc_file.name}")
                except Exception as e:
                    logger.error(f"ä¿å­˜ä¿®å¤åçš„æ–‡æ¡£æ•°æ®å¤±è´¥: {e}")
                    return False

            return True

        except Exception as e:
            logger.error(f"ä¿®å¤æ–‡æ¡£æ•°æ®å¤±è´¥: {e}")
            return False

    async def _create_lightweight_document(self, doc_data: dict):
        """åˆ›å»ºè½»é‡çº§æ–‡æ¡£å¯¹è±¡ï¼ˆä¸åŠ è½½å†…å®¹ï¼‰"""
        try:
            from src.domain.entities.document import Document, DocumentType

            # éªŒè¯åŸºæœ¬å¿…è¦å­—æ®µ
            if 'id' not in doc_data:
                logger.debug(f"æ–‡æ¡£æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: id")
                return None

            # æ£€æŸ¥æ–‡æ¡£ç±»å‹å­—æ®µï¼ˆå¯èƒ½æ˜¯typeæˆ–document_typeï¼‰
            doc_type_value = doc_data.get('type') or doc_data.get('document_type')
            if not doc_type_value:
                logger.debug(f"æ–‡æ¡£æ•°æ®ç¼ºå°‘æ–‡æ¡£ç±»å‹å­—æ®µ")
                return None

            # å®‰å…¨è½¬æ¢æ–‡æ¡£ç±»å‹
            try:
                if isinstance(doc_type_value, str):
                    doc_type = DocumentType(doc_type_value)
                else:
                    doc_type = DocumentType.CHAPTER
            except ValueError:
                logger.debug(f"æ— æ•ˆçš„æ–‡æ¡£ç±»å‹: {doc_type_value}, ä½¿ç”¨é»˜è®¤ç±»å‹")
                doc_type = DocumentType.CHAPTER

            # æ£€æŸ¥æ ‡é¢˜ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            title = self._extract_document_title(doc_data)

            # æå–å…ƒæ•°æ®
            metadata = self._extract_document_metadata(doc_data, title)

            # æå–ç»Ÿè®¡ä¿¡æ¯
            statistics = self._extract_document_statistics(doc_data)

            # ç¡®ä¿æ•°æ®ç»“æ„æ­£ç¡®
            normalized_data = {
                'id': doc_data['id'],
                'type': doc_type.value,
                'content': '',  # è½»é‡çº§å¯¹è±¡ä¸åŠ è½½å†…å®¹
                'project_id': doc_data.get('project_id', ''),
                'metadata': metadata,
                'statistics': statistics,
                'status': doc_data.get('status', 'draft'),
                'type_specific_data': doc_data.get('type_specific_data', {})
            }

            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            document = Document.from_dict(normalized_data)

            return document

        except Exception as e:
            logger.debug(f"åˆ›å»ºè½»é‡çº§æ–‡æ¡£å¯¹è±¡å¤±è´¥: {e}")
            return None

    def _extract_document_title(self, doc_data: dict) -> str:
        """æå–æ–‡æ¡£æ ‡é¢˜ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰"""
        # æ–¹æ³•1: ä»metadataä¸­è·å–
        if 'metadata' in doc_data and isinstance(doc_data['metadata'], dict):
            title = doc_data['metadata'].get('title', '')
            if title:
                return title

        # æ–¹æ³•2: ä»é¡¶çº§å­—æ®µè·å–
        title = doc_data.get('title', '')
        if title:
            return title

        # æ–¹æ³•3: ä»nameå­—æ®µè·å–ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
        title = doc_data.get('name', '')
        if title:
            return title

        # æ–¹æ³•4: ä½¿ç”¨é»˜è®¤æ ‡é¢˜
        doc_id = doc_data.get('id', 'unknown')
        return f"æ–‡æ¡£_{doc_id[:8]}"

    def _extract_document_metadata(self, doc_data: dict, title: str) -> dict:
        """æå–æ–‡æ¡£å…ƒæ•°æ®"""
        metadata = {
            'title': title,
            'description': '',
            'tags': [],
            'author': '',
            'created_at': '',
            'updated_at': ''
        }

        # ä»metadataå­—æ®µæå–
        if 'metadata' in doc_data and isinstance(doc_data['metadata'], dict):
            source_metadata = doc_data['metadata']
            metadata.update({
                'description': source_metadata.get('description', ''),
                'tags': source_metadata.get('tags', []),
                'author': source_metadata.get('author', ''),
                'created_at': source_metadata.get('created_at', ''),
                'updated_at': source_metadata.get('updated_at', '')
            })

        # å…¼å®¹æ—§æ ¼å¼çš„é¡¶çº§å­—æ®µ
        if not metadata['created_at']:
            metadata['created_at'] = doc_data.get('created_at', '')
        if not metadata['updated_at']:
            metadata['updated_at'] = doc_data.get('updated_at', '')

        return metadata

    def _extract_document_statistics(self, doc_data: dict) -> dict:
        """æå–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        default_stats = {
            'word_count': 0,
            'character_count': 0,
            'paragraph_count': 0,
            'sentence_count': 0,
            'reading_time_minutes': 0.0
        }

        # ä»statisticså­—æ®µæå–
        if 'statistics' in doc_data and isinstance(doc_data['statistics'], dict):
            stats = doc_data['statistics']
            default_stats.update({
                'word_count': max(0, stats.get('word_count', 0)),
                'character_count': max(0, stats.get('character_count', 0)),
                'paragraph_count': max(0, stats.get('paragraph_count', 0)),
                'sentence_count': max(0, stats.get('sentence_count', 0)),
                'reading_time_minutes': max(0.0, stats.get('reading_time_minutes', 0.0))
            })

        return default_stats

    def _clear_project_cache(self, project_id: str) -> None:
        """æ¸…ç†æŒ‡å®šé¡¹ç›®çš„ç¼“å­˜"""
        try:
            # æ¸…ç†é¡¹ç›®æ–‡æ¡£ç¼“å­˜
            cache_key = f"{self._cache_prefix}:project_docs:{project_id}"
            self.performance_manager.cache_delete(cache_key)
            logger.debug(f"âœ… å·²æ¸…ç†é¡¹ç›®æ–‡æ¡£ç¼“å­˜: {project_id}")

        except Exception as e:
            logger.debug(f"æ¸…ç†é¡¹ç›®ç¼“å­˜å¤±è´¥: {e}")

    def clear_all_cache(self) -> None:
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        try:
            logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†æ–‡æ¡£ä»“å‚¨çš„æ‰€æœ‰ç¼“å­˜")

            # æ¸…ç†æ—§çš„ç¼“å­˜ï¼ˆå‘åå…¼å®¹ï¼‰
            if hasattr(self, '_project_docs_cache'):
                self._project_docs_cache.clear()
                logger.debug("âœ… æ—§ç‰ˆé¡¹ç›®æ–‡æ¡£ç¼“å­˜å·²æ¸…é™¤")

            if hasattr(self, '_document_cache'):
                self._document_cache.clear()
                logger.debug("âœ… æ—§ç‰ˆæ–‡æ¡£ç¼“å­˜å·²æ¸…é™¤")

            # æ¸…ç†ç»Ÿä¸€æ€§èƒ½ç®¡ç†å™¨ä¸­çš„æ–‡æ¡£ç›¸å…³ç¼“å­˜
            if hasattr(self, 'performance_manager') and self.performance_manager:
                cache_prefix = getattr(self, '_cache_prefix', 'file_document_repo')

                # æ¸…é™¤æ‰€æœ‰é¡¹ç›®æ–‡æ¡£ç¼“å­˜
                # æ³¨æ„ï¼šç»Ÿä¸€æ€§èƒ½ç®¡ç†å™¨ä½¿ç”¨ä¸åŒçš„API
                try:
                    # æ¸…ç†ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
                    cache_stats = self.performance_manager.get_cache_stats()
                    logger.info(f"âœ… ç¼“å­˜æ¸…ç†å‰ç»Ÿè®¡: {cache_stats}")

                    # ç»Ÿä¸€æ€§èƒ½ç®¡ç†å™¨ä¼šè‡ªåŠ¨ç®¡ç†ç¼“å­˜æ¸…ç†
                    logger.info("âœ… ç»Ÿä¸€æ€§èƒ½ç®¡ç†å™¨ä¸­çš„æ–‡æ¡£ç¼“å­˜å·²æ¸…ç†")
                except Exception as e:
                    logger.warning(f"æ¸…ç†ç»Ÿä¸€æ€§èƒ½ç®¡ç†å™¨ç¼“å­˜æ—¶å‡ºé”™: {e}")

            logger.info("ğŸ‰ æ–‡æ¡£ä»“å‚¨ç¼“å­˜æ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.error(f"æ¸…ç†æ‰€æœ‰ç¼“å­˜å¤±è´¥: {e}")

    async def list_by_type(
        self,
        document_type: DocumentType,
        project_id: Optional[str] = None
    ) -> List[Document]:
        """æ ¹æ®ç±»å‹åˆ—å‡ºæ–‡æ¡£"""
        documents = []

        for doc_file in self.base_path.glob("*.json"):
            try:
                doc_data = await self.file_ops.load_json_cached(
                    file_path=doc_file,
                    cache_key=f"{self._cache_prefix}:meta:{doc_file.stem}",
                    cache_ttl=300
                )

                if (doc_data.get('document_type') == document_type.value and
                    (project_id is None or doc_data.get('project_id') == project_id)):
                    document = await self.load(doc_data['id'])
                    if document:
                        documents.append(document)
            except Exception as e:
                logger.warning(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {doc_file}, {e}")

        return documents

    async def list_by_status(
        self,
        status: DocumentStatus,
        project_id: Optional[str] = None
    ) -> List[Document]:
        """æ ¹æ®çŠ¶æ€åˆ—å‡ºæ–‡æ¡£"""
        documents = []

        for doc_file in self.base_path.glob("*.json"):
            try:
                doc_data = await self.file_ops.load_json_cached(
                    file_path=doc_file,
                    cache_key=f"{self._cache_prefix}:meta:{doc_file.stem}",
                    cache_ttl=300
                )

                if (doc_data.get('status') == status.value and
                    (project_id is None or doc_data.get('project_id') == project_id)):
                    document = await self.load(doc_data['id'])
                    if document:
                        documents.append(document)
            except Exception as e:
                logger.warning(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {doc_file}, {e}")

        return documents

    async def search(
        self,
        query: str,
        project_id: Optional[str] = None
    ) -> List[Document]:
        """æœç´¢æ–‡æ¡£"""
        documents = []
        query_lower = query.lower()

        for doc_file in self.base_path.glob("*.json"):
            try:
                doc_data = await self.file_ops.load_json_cached(
                    file_path=doc_file,
                    cache_key=f"{self._cache_prefix}:meta:{doc_file.stem}",
                    cache_ttl=300
                )

                if project_id and doc_data.get('project_id') != project_id:
                    continue

                # æœç´¢æ ‡é¢˜å’Œæè¿°
                metadata = doc_data.get('metadata', {})
                title = metadata.get('title', '').lower()
                description = metadata.get('description', '').lower()
                tags = metadata.get('tags', [])

                if (query_lower in title or
                    query_lower in description or
                    any(query_lower in tag.lower() for tag in tags)):
                    document = await self.load(doc_data['id'])
                    if document:
                        documents.append(document)
            except Exception as e:
                logger.warning(f"æœç´¢æ–‡æ¡£å¤±è´¥: {doc_file}, {e}")

        return documents

    async def search_content(
        self,
        query: str,
        project_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£å†…å®¹"""
        results = []
        query_lower = query.lower()

        for content_file in self.base_path.glob("*_content.txt"):
            try:
                document_id = content_file.stem.replace('_content', '')

                # æ£€æŸ¥é¡¹ç›®ID
                if project_id:
                    # åœ¨æ‰€æœ‰ç±»å‹å­ç›®å½•ä¸­å°è¯•è¯»å–å…ƒæ•°æ®
                    doc_data = None
                    for sub in set(DOC_TYPE_DIRS.values()) | {""}:
                        base = self.base_path / sub if sub else self.base_path
                        test_doc = base / f"{document_id}.json"
                        if test_doc.exists():
                            doc_data = await self.file_ops.load_json_cached(
                                file_path=test_doc,
                                cache_key=f"{self._cache_prefix}:meta:{test_doc.stem}",
                                cache_ttl=300
                            )
                            break
                    if doc_data and doc_data.get('project_id') != project_id:
                        continue

                # æœç´¢å†…å®¹
                content = await self._read_text_file_safe(content_file)

                if query_lower in content.lower():
                    # æŸ¥æ‰¾åŒ¹é…çš„ä¸Šä¸‹æ–‡
                    lines = content.split('\n')
                    matches = []

                    for i, line in enumerate(lines):
                        if query_lower in line.lower():
                            # è·å–ä¸Šä¸‹æ–‡ï¼ˆå‰åå„2è¡Œï¼‰
                            start = max(0, i - 2)
                            end = min(len(lines), i + 3)
                            context = '\n'.join(lines[start:end])

                            matches.append({
                                "line_number": i + 1,
                                "line": line.strip(),
                                "context": context
                            })

                    if matches:
                        results.append({
                            "document_id": document_id,
                            "matches": matches
                        })

            except Exception as e:
                logger.warning(f"æœç´¢å†…å®¹å¤±è´¥: {content_file}, {e}")

        return results

    async def get_recent_documents(
        self,
        limit: int = 10,
        project_id: Optional[str] = None
    ) -> List[Document]:
        """è·å–æœ€è¿‘ç¼–è¾‘çš„æ–‡æ¡£"""
        documents = []

        for doc_file in self.base_path.glob("*.json"):
            try:
                doc_data = await self.file_ops.load_json_cached(
                    file_path=doc_file,
                    cache_key=f"{self._cache_prefix}:meta:{doc_file.stem}",
                    cache_ttl=300
                )

                if project_id and doc_data.get('project_id') != project_id:
                    continue

                document = await self.load(doc_data['id'])
                if document:
                    documents.append(document)
            except Exception as e:
                logger.warning(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {doc_file}, {e}")

        # æŒ‰æ›´æ–°æ—¶é—´æ’åº
        documents.sort(
            key=lambda d: d.metadata.updated_at,
            reverse=True
        )

        return documents[:limit]

    async def update_content(self, document_id: str, content: str) -> bool:
        """æ›´æ–°æ–‡æ¡£å†…å®¹"""
        try:
            document = await self.load(document_id)
            if not document:
                return False

            document.content = content
            return await self.save(document)

        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            return False

    async def update_metadata(
        self,
        document_id: str,
        metadata: Dict[str, Any]
    ) -> bool:
        """æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®"""
        try:
            document = await self.load(document_id)
            if not document:
                return False

            # æ›´æ–°å…ƒæ•°æ®
            for key, value in metadata.items():
                if hasattr(document.metadata, key):
                    setattr(document.metadata, key, value)

            document.metadata.updated_at = datetime.now()
            return await self.save(document)

        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®å¤±è´¥: {e}")
            return False

    async def get_word_count(self, document_id: str) -> int:
        """è·å–æ–‡æ¡£å­—æ•°"""
        try:
            content_path = self._get_content_path(document_id)
            if not content_path.exists():
                return 0

            content = await self._read_text_file_safe(content_path)

            return len(content.split()) if content.strip() else 0

        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£å­—æ•°å¤±è´¥: {e}")
            return 0

    async def get_statistics(self, document_id: str) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        try:
            document = await self.load(document_id)
            if not document:
                return {}

            return {
                "document_id": document_id,
                "title": document.title,
                "word_count": document.statistics.word_count,
                "character_count": document.statistics.character_count,
                "paragraph_count": document.statistics.paragraph_count,
                "sentence_count": document.statistics.sentence_count,
                "reading_time_minutes": document.statistics.reading_time_minutes,
                "last_edit_time": document.statistics.last_edit_time.isoformat() if document.statistics.last_edit_time else None,
                "edit_count": document.statistics.edit_count,
                "created_at": document.metadata.created_at.isoformat(),
                "updated_at": document.metadata.updated_at.isoformat(),
            }

        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    # ç‰ˆæœ¬ç®¡ç†æ–¹æ³•ï¼ˆç®€å•å®ç°ï¼‰
    async def cleanup_old_versions(self, document_id: str, keep_count: int = 10) -> bool:
        """æ¸…ç†æ—§ç‰ˆæœ¬"""
        try:
            # é¦–å…ˆå°è¯•é»˜è®¤è·¯å¾„
            doc_path = self._get_document_path(document_id)

            # å¦‚æœé»˜è®¤è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨é¡¹ç›®ç›®å½•ä¸­æŸ¥æ‰¾
            if not doc_path.exists():
                found_paths = await self._find_document_in_projects(document_id)
                if found_paths:
                    doc_path, _ = found_paths
                else:
                    logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨ï¼Œæ— æ³•æ¸…ç†ç‰ˆæœ¬: {document_id}")
                    return False

            # ç‰ˆæœ¬æ–‡ä»¶å­˜å‚¨åœ¨åŒç›®å½•ä¸‹ï¼Œä»¥ {document_id}_v{timestamp}.txt å‘½å
            doc_dir = doc_path.parent
            version_pattern = f"{document_id}_v*.txt"
            version_files = list(doc_dir.glob(version_pattern))

            if len(version_files) <= keep_count:
                logger.debug(f"ç‰ˆæœ¬æ•°é‡({len(version_files)})æœªè¶…è¿‡ä¿ç•™æ•°é‡({keep_count})ï¼Œæ— éœ€æ¸…ç†")
                return True

            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„ç‰ˆæœ¬
            version_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
            files_to_delete = version_files[keep_count:]

            deleted_count = 0
            for version_file in files_to_delete:
                try:
                    version_file.unlink()
                    deleted_count += 1
                    logger.debug(f"åˆ é™¤æ—§ç‰ˆæœ¬æ–‡ä»¶: {version_file.name}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤ç‰ˆæœ¬æ–‡ä»¶å¤±è´¥ {version_file}: {e}")

            logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§ç‰ˆæœ¬æ–‡ä»¶")
            return True

        except Exception as e:
            logger.error(f"æ¸…ç†æ—§ç‰ˆæœ¬å¤±è´¥: {e}")
            return False

    async def delete_version(self, document_id: str, version_id: str) -> bool:
        """åˆ é™¤æŒ‡å®šç‰ˆæœ¬"""
        try:
            # é¦–å…ˆå°è¯•é»˜è®¤è·¯å¾„
            doc_path = self._get_document_path(document_id)

            # å¦‚æœé»˜è®¤è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨é¡¹ç›®ç›®å½•ä¸­æŸ¥æ‰¾
            if not doc_path.exists():
                found_paths = await self._find_document_in_projects(document_id)
                if found_paths:
                    doc_path, _ = found_paths
                else:
                    logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨: {document_id}")
                    return False

            doc_dir = doc_path.parent
            version_file = doc_dir / f"{document_id}_v{version_id}.txt"

            if not version_file.exists():
                logger.warning(f"ç‰ˆæœ¬æ–‡ä»¶ä¸å­˜åœ¨: {version_file}")
                return False

            version_file.unlink()
            logger.info(f"åˆ é™¤ç‰ˆæœ¬æˆåŠŸ: {document_id} ç‰ˆæœ¬ {version_id}")
            return True

        except Exception as e:
            logger.error(f"åˆ é™¤ç‰ˆæœ¬å¤±è´¥: {e}")
            return False

    async def _create_version_with_path(self, document_id: str, content: str, doc_path: Path, description: str = "") -> Optional[str]:
        """ä½¿ç”¨æŒ‡å®šè·¯å¾„åˆ›å»ºæ–‡æ¡£ç‰ˆæœ¬ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        try:
            # ç”Ÿæˆç‰ˆæœ¬IDï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
            version_id = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ç²¾ç¡®åˆ°æ¯«ç§’

            # åˆ›å»ºç‰ˆæœ¬æ–‡ä»¶
            doc_dir = doc_path.parent
            version_file = doc_dir / f"{document_id}_v{version_id}.txt"

            # ä¿å­˜ç‰ˆæœ¬å†…å®¹ï¼ˆå§”æ‰˜ç»Ÿä¸€å®ç°ï¼ŒåŸå­å†™å…¥+å¤‡ä»½ï¼‰
            await self.file_ops.save_text_atomic(version_file, content, create_backup=True)

            # åˆ›å»ºç‰ˆæœ¬å…ƒæ•°æ®æ–‡ä»¶
            version_meta_file = doc_dir / f"{document_id}_v{version_id}.meta.json"
            version_meta = {
                "version_id": version_id,
                "document_id": document_id,
                "created_at": datetime.now().isoformat(),
                "description": description
            }

            await self.file_ops.save_json_atomic(
                file_path=version_meta_file,
                data=version_meta,
                create_backup=False,
                cache_key=f"{self._cache_prefix}:version_meta:{document_id}:{version_id}",
                cache_ttl=300
            )

            logger.debug(f"ç‰ˆæœ¬åˆ›å»ºæˆåŠŸ: {document_id} -> {version_id}")
            return version_id

        except Exception as e:
            logger.error(f"åˆ›å»ºç‰ˆæœ¬å¤±è´¥: {e}")
            return None

    async def create_version(self, document_id: str, content: str, description: str = "") -> Optional[str]:
        """åˆ›å»ºæ–‡æ¡£ç‰ˆæœ¬"""
        try:
            # é¦–å…ˆå°è¯•é»˜è®¤è·¯å¾„
            doc_path = self._get_document_path(document_id)

            # å¦‚æœé»˜è®¤è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨é¡¹ç›®ç›®å½•ä¸­æŸ¥æ‰¾
            if not doc_path.exists():
                doc_path, _ = await self._find_document_in_projects(document_id)
                if not doc_path or not doc_path.exists():
                    logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨: {document_id}")
                    return None

            return await self._create_version_with_path(document_id, content, doc_path, description)

        except Exception as e:
            logger.error(f"åˆ›å»ºç‰ˆæœ¬å¤±è´¥: {e}")
            return None

    async def get_version_diff(self, document_id: str, version1_id: str, version2_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ç‰ˆæœ¬å·®å¼‚"""
        try:
            doc_path = self._get_document_path(document_id)
            if not doc_path.exists():
                logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨: {document_id}")
                return None

            doc_dir = doc_path.parent

            # è·å–ä¸¤ä¸ªç‰ˆæœ¬çš„å†…å®¹
            version1_file = doc_dir / f"{document_id}_v{version1_id}.txt"
            version2_file = doc_dir / f"{document_id}_v{version2_id}.txt"

            if not version1_file.exists():
                logger.warning(f"ç‰ˆæœ¬1æ–‡ä»¶ä¸å­˜åœ¨: {version1_file}")
                return None

            if not version2_file.exists():
                logger.warning(f"ç‰ˆæœ¬2æ–‡ä»¶ä¸å­˜åœ¨: {version2_file}")
                return None

            # è¯»å–ç‰ˆæœ¬å†…å®¹
            content1 = await self._read_text_file_safe(version1_file)
            content2 = await self._read_text_file_safe(version2_file)

            # ç®€å•çš„å·®å¼‚åˆ†æ
            lines1 = content1.splitlines()
            lines2 = content2.splitlines()

            # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            diff_info = {
                "document_id": document_id,
                "version1_id": version1_id,
                "version2_id": version2_id,
                "version1_lines": len(lines1),
                "version2_lines": len(lines2),
                "version1_chars": len(content1),
                "version2_chars": len(content2),
                "lines_added": 0,
                "lines_removed": 0,
                "lines_modified": 0,
                "changes": []
            }

            # ç®€å•çš„é€è¡Œæ¯”è¾ƒ
            max_lines = max(len(lines1), len(lines2))
            for i in range(max_lines):
                line1 = lines1[i] if i < len(lines1) else None
                line2 = lines2[i] if i < len(lines2) else None

                if line1 is None:
                    # æ–°å¢è¡Œ
                    diff_info["lines_added"] += 1
                    diff_info["changes"].append({
                        "type": "added",
                        "line_number": i + 1,
                        "content": line2
                    })
                elif line2 is None:
                    # åˆ é™¤è¡Œ
                    diff_info["lines_removed"] += 1
                    diff_info["changes"].append({
                        "type": "removed",
                        "line_number": i + 1,
                        "content": line1
                    })
                elif line1 != line2:
                    # ä¿®æ”¹è¡Œ
                    diff_info["lines_modified"] += 1
                    diff_info["changes"].append({
                        "type": "modified",
                        "line_number": i + 1,
                        "old_content": line1,
                        "new_content": line2
                    })

            logger.info(f"ç‰ˆæœ¬å·®å¼‚åˆ†æå®Œæˆ: {document_id} {version1_id} vs {version2_id}")
            return diff_info

        except Exception as e:
            logger.error(f"è·å–ç‰ˆæœ¬å·®å¼‚å¤±è´¥: {e}")
            return None

    async def restore_version(self, document_id: str, version_id: str) -> bool:
        """æ¢å¤åˆ°æŒ‡å®šç‰ˆæœ¬"""
        try:
            doc_path = self._get_document_path(document_id)
            if not doc_path.exists():
                logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨: {document_id}")
                return False

            doc_dir = doc_path.parent
            version_file = doc_dir / f"{document_id}_v{version_id}.txt"

            if not version_file.exists():
                logger.warning(f"ç‰ˆæœ¬æ–‡ä»¶ä¸å­˜åœ¨: {version_file}")
                return False

            # è¯»å–ç‰ˆæœ¬å†…å®¹
            version_content = await self._read_text_file_safe(version_file)

            # è·å–å½“å‰æ–‡æ¡£
            document = await self.get_by_id(document_id)
            if not document:
                logger.warning(f"æ— æ³•è·å–æ–‡æ¡£: {document_id}")
                return False

            # åœ¨æ¢å¤å‰åˆ›å»ºå½“å‰ç‰ˆæœ¬çš„å¤‡ä»½
            current_content_path = doc_dir / f"{document_id}_content.txt"
            if current_content_path.exists():
                current_content = await self._read_text_file_safe(current_content_path)

                # åˆ›å»ºæ¢å¤å‰çš„å¤‡ä»½
                backup_version_id = await self.create_version(
                    document_id,
                    current_content,
                    f"æ¢å¤å‰å¤‡ä»½ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                )
                if backup_version_id:
                    logger.info(f"å·²åˆ›å»ºæ¢å¤å‰å¤‡ä»½: {backup_version_id}")

            # æ›´æ–°æ–‡æ¡£å†…å®¹
            document.content = version_content
            document.updated_at = datetime.now()

            # ä¿å­˜æ–‡æ¡£
            success = await self.save(document)

            if success:
                logger.info(f"ç‰ˆæœ¬æ¢å¤æˆåŠŸ: {document_id} -> ç‰ˆæœ¬ {version_id}")
                return True
            else:
                logger.error(f"ç‰ˆæœ¬æ¢å¤å¤±è´¥: ä¿å­˜æ–‡æ¡£æ—¶å‡ºé”™")
                return False

        except Exception as e:
            logger.error(f"æ¢å¤ç‰ˆæœ¬å¤±è´¥: {e}")
            return False

    async def load_content_streaming(self, document_id: str, chunk_size: int = 8192) -> AsyncGenerator[str, None]:
        """
        æµå¼åŠ è½½æ–‡æ¡£å†…å®¹

        åˆ†å—å¼‚æ­¥åŠ è½½å¤§æ–‡æ¡£å†…å®¹ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜ã€‚
        é€‚ç”¨äºè¶…å¤§æ–‡æ¡£çš„æ€§èƒ½ä¼˜åŒ–ã€‚

        Args:
            document_id: æ–‡æ¡£ID
            chunk_size: æ¯ä¸ªå—çš„å¤§å°ï¼ˆå­—èŠ‚ï¼‰

        Yields:
            str: æ–‡æ¡£å†…å®¹å—
        """
        try:
            # è·å–å†…å®¹æ–‡ä»¶è·¯å¾„
            content_path = self._get_content_path(document_id)

            # å¦‚æœé»˜è®¤è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨é¡¹ç›®ä¸­æŸ¥æ‰¾
            if not content_path.exists():
                _, content_path = await self._find_document_in_projects(document_id)
                if not content_path or not content_path.exists():
                    logger.warning(f"æ–‡æ¡£å†…å®¹æ–‡ä»¶ä¸å­˜åœ¨: {document_id}")
                    return

            logger.info(f"å¼€å§‹æµå¼åŠ è½½æ–‡æ¡£å†…å®¹: {document_id}, å—å¤§å°: {chunk_size}")

            # æµå¼è¯»å–æ–‡ä»¶ï¼ˆæ”¯æŒç¼–ç å›é€€ï¼‰
            chunk_count = 0
            try:
                # ä½¿ç”¨ç»Ÿä¸€æ–‡ä»¶æ“ä½œè¿›è¡Œæµå¼è¯»å–
                iterator = await self.file_ops.stream_text(content_path, chunk_size)
                if iterator is None:
                    logger.error(f"æ— æ³•æµå¼è¯»å–æ–‡ä»¶: {content_path}")
                    return
                for chunk in iterator:
                    chunk_count += 1
                    logger.debug(f"æµå¼åŠ è½½å— {chunk_count}: {len(chunk)} å­—ç¬¦")
                    yield chunk
                    await asyncio.sleep(0.001)
            except Exception as e:
                logger.error(f"æµå¼è¯»å–å¤±è´¥: {e}")
                return

            logger.info(f"æµå¼åŠ è½½å®Œæˆ: {document_id}, æ€»å—æ•°: {chunk_count}")

        except UnicodeDecodeError as e:
            logger.warning(f"æ–‡æ¡£ç¼–ç é”™è¯¯ï¼Œå°è¯•å…¶ä»–ç¼–ç : {e}")
            # ä½¿ç”¨ç»Ÿä¸€æ–‡ä»¶æ“ä½œé‡è¯•æµå¼è¯»å–
            try:
                iterator = await self.file_ops.stream_text(content_path, chunk_size)
                if iterator is None:
                    return
                for chunk in iterator:
                    yield chunk
                    await asyncio.sleep(0.001)
            except Exception as fallback_error:
                logger.error(f"æµå¼åŠ è½½å¤±è´¥ï¼ˆç¼–ç é—®é¢˜ï¼‰: {fallback_error}")
                return

        except Exception as e:
            logger.error(f"æµå¼åŠ è½½æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            return

    async def load_content_by_lines(self, document_id: str, start_line: int = 0, line_count: int = 1000) -> Optional[List[str]]:
        """
        æŒ‰è¡ŒåŠ è½½æ–‡æ¡£å†…å®¹

        åŠ è½½æŒ‡å®šè¡ŒèŒƒå›´çš„æ–‡æ¡£å†…å®¹ï¼Œç”¨äºè™šæ‹ŸåŒ–æ¸²æŸ“ã€‚

        Args:
            document_id: æ–‡æ¡£ID
            start_line: èµ·å§‹è¡Œå·ï¼ˆä»0å¼€å§‹ï¼‰
            line_count: è¦åŠ è½½çš„è¡Œæ•°

        Returns:
            List[str]: æŒ‡å®šèŒƒå›´çš„æ–‡æ¡£è¡Œï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            # è·å–å†…å®¹æ–‡ä»¶è·¯å¾„
            content_path = self._get_content_path(document_id)

            if not content_path.exists():
                _, content_path = await self._find_document_in_projects(document_id)
                if not content_path or not content_path.exists():
                    logger.warning(f"æ–‡æ¡£å†…å®¹æ–‡ä»¶ä¸å­˜åœ¨: {document_id}")
                    return None

            logger.debug(f"æŒ‰è¡ŒåŠ è½½æ–‡æ¡£å†…å®¹: {document_id}, è¡Œ{start_line}-{start_line + line_count}")

            # è¯»å–æŒ‡å®šè¡ŒèŒƒå›´ï¼ˆç»Ÿä¸€å®ç°ï¼‰
            lines = await self.file_ops.load_lines_safe(
                content_path, start_line=start_line, line_count=line_count
            )
            if lines is None:
                logger.error(f"æ— æ³•æŒ‰è¡Œè¯»å–æ–‡æ¡£å†…å®¹: {content_path}")
                return None

            logger.debug(f"æŒ‰è¡ŒåŠ è½½å®Œæˆ: {len(lines)} è¡Œ")
            return lines

        except UnicodeDecodeError:
            # å…¨éƒ¨äº¤ç”±ç»Ÿä¸€å®ç°å¤„ç†ç¼–ç å›é€€
            try:
                lines = await self.file_ops.load_lines_safe(
                    content_path, start_line=start_line, line_count=line_count
                )
                return lines
            except Exception as e:
                logger.error(f"æŒ‰è¡ŒåŠ è½½å¤±è´¥ï¼ˆç¼–ç é—®é¢˜ï¼‰: {e}")
                return None
        except Exception as e:
            logger.error(f"æŒ‰è¡ŒåŠ è½½æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            return None

    async def load_metadata_only(self, document_id: str) -> Optional[Document]:
        """
        åªåŠ è½½æ–‡æ¡£å…ƒæ•°æ®ï¼Œä¸åŠ è½½å†…å®¹

        ç”¨äºå¿«é€Ÿè·å–æ–‡æ¡£ä¿¡æ¯è€Œä¸åŠ è½½å¤§é‡å†…å®¹åˆ°å†…å­˜ã€‚

        Args:
            document_id: æ–‡æ¡£ID

        Returns:
            Document: åªåŒ…å«å…ƒæ•°æ®çš„æ–‡æ¡£å¯¹è±¡ï¼Œcontentä¸ºç©ºå­—ç¬¦ä¸²
        """
        try:
            # è·å–æ–‡æ¡£å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            doc_path = self._get_document_path(document_id)

            if not doc_path.exists():
                doc_path, _ = await self._find_document_in_projects(document_id)
                if not doc_path or not doc_path.exists():
                    return None

            # åªåŠ è½½å…ƒæ•°æ®
            doc_data = await self.file_ops.load_json_cached(
                file_path=doc_path,
                cache_key=f"{self._cache_prefix}:meta:{doc_path.stem}",
                cache_ttl=300
            )

            # éªŒè¯æ•°æ®æ ¼å¼
            if not isinstance(doc_data, dict):
                logger.error(f"æ–‡æ¡£å…ƒæ•°æ®æ ¼å¼æ— æ•ˆ: {doc_path}")
                return None

            # ä½¿ç”¨ç»Ÿä¸€çš„æ„å»ºæ–¹æ³•ï¼ˆä¸åŠ è½½å†…å®¹ï¼‰
            document = self._build_document_from_data(doc_data, "")
            if not document:
                return None

            # è®¾ç½®é¢å¤–çš„å…ƒæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'metadata' in doc_data:
                metadata = doc_data['metadata']
                document.word_count = metadata.get('word_count', 0)
                document.character_count = metadata.get('character_count', 0)

                # æ—¶é—´æˆ³
                if 'created_at' in metadata:
                    document.created_at = datetime.fromisoformat(metadata['created_at'])
                if 'updated_at' in metadata:
                    document.updated_at = datetime.fromisoformat(metadata['updated_at'])

            logger.debug(f"å…ƒæ•°æ®åŠ è½½å®Œæˆ: {document.title}")
            return document

        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æ¡£å…ƒæ•°æ®å¤±è´¥: {e}")
            return None

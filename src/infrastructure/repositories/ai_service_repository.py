#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæœåŠ¡ä»“å‚¨å®ç°

é›†æˆå¤šç§AIæœåŠ¡æä¾›å•†çš„å®ç°
"""

import asyncio
import aiohttp
import json
import hashlib
from typing import List, Optional, Dict, Any, AsyncGenerator
from datetime import datetime, timedelta

from src.domain.repositories.ai_service_repository import IAIServiceRepository
from src.shared.utils.logger import get_logger
from config.settings import Settings

logger = get_logger(__name__)


class AIServiceRepository(IAIServiceRepository):
    """AIæœåŠ¡ä»“å‚¨å®ç°"""
    
    def __init__(self, settings: Optional[Settings] = None):
        self.settings = settings or Settings()
        self._session: Optional[aiohttp.ClientSession] = None
        self._client_manager: Optional['AIClientManager'] = None

        # ç¼“å­˜
        self._response_cache: Dict[str, Dict[str, Any]] = {}
        self._cache_ttl = 3600  # 1å°æ—¶
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """è·å–HTTPä¼šè¯"""
        if self._session is None or self._session.closed:
            timeout = aiohttp.ClientTimeout(total=self.settings.ai_service.timeout)
            self._session = aiohttp.ClientSession(timeout=timeout)
        return self._session

    def _get_client_manager(self):
        """è·å–AIå®¢æˆ·ç«¯ç®¡ç†å™¨ï¼ˆå•ä¾‹ï¼‰"""
        if self._client_manager is None:
            from src.infrastructure.ai_clients.openai_client import AIClientManager
            self._client_manager = AIClientManager()
        return self._client_manager

    def _get_cache_key(self, prompt: str, context: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{prompt}|{context}|{json.dumps(kwargs, sort_keys=True)}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _is_cache_valid(self, cache_entry: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if 'timestamp' not in cache_entry:
            return False
        
        cache_time = datetime.fromisoformat(cache_entry['timestamp'])
        return datetime.now() - cache_time < timedelta(seconds=self._cache_ttl)
    
    async def generate_text(
        self, 
        prompt: str, 
        context: str = "",
        max_tokens: int = 2000,
        temperature: float = 0.7,
        model: Optional[str] = None
    ) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        try:
            logger.info(f"ğŸª AIä»“å‚¨å¼€å§‹ç”Ÿæˆæ–‡æœ¬ï¼Œæç¤ºè¯é•¿åº¦: {len(prompt)}")
            logger.debug(f"å‚æ•° - max_tokens: {max_tokens}, temperature: {temperature}, model: {model}")

            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._get_cache_key(prompt, context, max_tokens=max_tokens, temperature=temperature, model=model)
            if cache_key in self._response_cache:
                cache_entry = self._response_cache[cache_key]
                if self._is_cache_valid(cache_entry):
                    logger.info("ğŸ’¾ ä½¿ç”¨ç¼“å­˜çš„AIå“åº”")
                    return cache_entry['response']
                else:
                    logger.debug("ğŸ—‘ï¸ ç¼“å­˜å·²è¿‡æœŸï¼Œåˆ é™¤ç¼“å­˜æ¡ç›®")
                    del self._response_cache[cache_key]

            # ä½¿ç”¨å•ä¾‹AIå®¢æˆ·ç«¯ç®¡ç†å™¨
            logger.debug("ğŸ”Œ è·å–AIå®¢æˆ·ç«¯ç®¡ç†å™¨...")
            client_manager = self._get_client_manager()
            logger.debug(f"å®¢æˆ·ç«¯ç®¡ç†å™¨ç±»å‹: {type(client_manager)}")

            logger.info("ğŸ“¡ è°ƒç”¨AIå®¢æˆ·ç«¯ç”Ÿæˆæ–‡æœ¬...")
            response = await client_manager.simple_generate(
                prompt=prompt,
                context=context,
                max_tokens=max_tokens,
                temperature=temperature,
                model=model
            )

            logger.info(f"âœ… AIå®¢æˆ·ç«¯å“åº”æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response) if response else 0}")
            logger.debug(f"å“åº”å†…å®¹: {response[:100] if response else 'None'}...")

            # ç¼“å­˜å“åº”
            self._response_cache[cache_key] = {
                'response': response,
                'timestamp': datetime.now().isoformat()
            }
            logger.debug("ğŸ’¾ å“åº”å·²ç¼“å­˜")

            return response
            
        except Exception as e:
            logger.error(f"AIæ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            raise Exception(f"AIæœåŠ¡ä¸å¯ç”¨: {str(e)}")
    
    async def generate_text_stream(
        self,
        prompt: str,
        context: str = "",
        max_tokens: int = 2000,
        temperature: float = 0.7,
        model: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆæ–‡æœ¬"""
        try:
            logger.info(f"ğŸŒŠ AIä»“å‚¨å¼€å§‹æµå¼ç”Ÿæˆï¼Œæç¤ºè¯é•¿åº¦: {len(prompt)}")
            logger.debug(f"å‚æ•° - max_tokens: {max_tokens}, temperature: {temperature}, model: {model}")

            # ä½¿ç”¨å•ä¾‹AIå®¢æˆ·ç«¯ç®¡ç†å™¨
            logger.debug("ğŸ”Œ è·å–AIå®¢æˆ·ç«¯ç®¡ç†å™¨...")
            client_manager = self._get_client_manager()

            logger.info("ğŸ“¡ å¼€å§‹æµå¼ç”Ÿæˆ...")
            chunk_count = 0
            total_length = 0

            async for chunk in client_manager.stream_generate(
                prompt=prompt,
                context=context,
                max_tokens=max_tokens,
                temperature=temperature,
                model=model
            ):
                chunk_count += 1
                chunk_length = len(chunk) if chunk else 0
                total_length += chunk_length

                if chunk_count <= 3:
                    logger.debug(f"ğŸ“ ä»“å‚¨æ¥æ”¶ç¬¬ {chunk_count} ä¸ªå—: {chunk[:50]}...")
                elif chunk_count % 10 == 0:
                    logger.debug(f"ğŸ“Š ä»“å‚¨å·²å¤„ç† {chunk_count} ä¸ªå—ï¼Œæ€»é•¿åº¦: {total_length}")

                yield chunk

            logger.info(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œå…±å¤„ç† {chunk_count} ä¸ªå—ï¼Œæ€»é•¿åº¦: {total_length}")

        except Exception as e:
            logger.error(f"âŒ AIæµå¼ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.debug(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")

            # å¦‚æœæ˜¯å¼‚æ­¥è¿­ä»£å™¨é”™è¯¯ï¼Œå°è¯•é™çº§åˆ°éæµå¼ç”Ÿæˆ
            if "'async for' requires an object with __aiter__ method" in str(e):
                logger.warning("æ£€æµ‹åˆ°å¼‚æ­¥è¿­ä»£å™¨é”™è¯¯ï¼Œå°è¯•é™çº§åˆ°éæµå¼ç”Ÿæˆ")
                try:
                    # é™çº§åˆ°éæµå¼ç”Ÿæˆ
                    result = await client_manager.simple_generate(
                        prompt=prompt,
                        context=context,
                        max_tokens=max_tokens,
                        temperature=temperature
                    )

                    # åˆ†å—è¿”å›ç»“æœæ¨¡æ‹Ÿæµå¼è¾“å‡º
                    if result:
                        chunk_size = 15  # æ¯æ¬¡è¿”å›15ä¸ªå­—ç¬¦
                        chunk_count = 0
                        for i in range(0, len(result), chunk_size):
                            chunk = result[i:i + chunk_size]
                            chunk_count += 1

                            if chunk_count <= 3:
                                logger.debug(f"ğŸ“ ä»“å‚¨æ¨¡æ‹Ÿç¬¬ {chunk_count} ä¸ªå—: {chunk[:50]}...")

                            yield chunk

                            # æ·»åŠ å°å»¶è¿Ÿæ¨¡æ‹Ÿæµå¼æ•ˆæœ
                            import asyncio
                            await asyncio.sleep(0.05)

                    logger.info(f"âœ… æ¨¡æ‹Ÿæµå¼ç”Ÿæˆå®Œæˆï¼Œå…±å¤„ç† {chunk_count} ä¸ªå—")
                    return

                except Exception as fallback_error:
                    logger.error(f"é™çº§åˆ°éæµå¼ç”Ÿæˆä¹Ÿå¤±è´¥: {fallback_error}")

            raise Exception(f"AIæµå¼æœåŠ¡ä¸å¯ç”¨: {str(e)}")
    
    async def _generate_with_openai(
        self, 
        prompt: str, 
        context: str,
        max_tokens: int,
        temperature: float,
        model: Optional[str]
    ) -> str:
        """ä½¿ç”¨OpenAIç”Ÿæˆæ–‡æœ¬"""
        if not self.settings.ai_service.openai_api_key:
            raise ValueError("OpenAI APIå¯†é’¥æœªé…ç½®")
        
        session = await self._get_session()
        
        headers = {
            "Authorization": f"Bearer {self.settings.ai_service.openai_api_key}",
            "Content-Type": "application/json"
        }
        
        messages = []
        if context:
            messages.append({"role": "system", "content": context})
        messages.append({"role": "user", "content": prompt})
        
        data = {
            "model": model or self.settings.ai_service.openai_model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        url = f"{self.settings.ai_service.openai_base_url}/chat/completions"
        
        async with session.post(url, headers=headers, json=data) as response:
            if response.status == 200:
                result = await response.json()
                return result["choices"][0]["message"]["content"]
            else:
                error_text = await response.text()
                raise Exception(f"OpenAI APIé”™è¯¯: {response.status}, {error_text}")
    
    async def _generate_with_deepseek(
        self, 
        prompt: str, 
        context: str,
        max_tokens: int,
        temperature: float,
        model: Optional[str]
    ) -> str:
        """ä½¿ç”¨DeepSeekç”Ÿæˆæ–‡æœ¬"""
        if not self.settings.ai_service.deepseek_api_key:
            raise ValueError("DeepSeek APIå¯†é’¥æœªé…ç½®")
        
        session = await self._get_session()
        
        headers = {
            "Authorization": f"Bearer {self.settings.ai_service.deepseek_api_key}",
            "Content-Type": "application/json"
        }
        
        messages = []
        if context:
            messages.append({"role": "system", "content": context})
        messages.append({"role": "user", "content": prompt})
        
        data = {
            "model": model or self.settings.ai_service.deepseek_model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        url = f"{self.settings.ai_service.deepseek_base_url}/chat/completions"
        
        async with session.post(url, headers=headers, json=data) as response:
            if response.status == 200:
                result = await response.json()
                return result["choices"][0]["message"]["content"]
            else:
                error_text = await response.text()
                raise Exception(f"DeepSeek APIé”™è¯¯: {response.status}, {error_text}")
    

    
    async def _generate_stream_openai(
        self,
        prompt: str,
        context: str,
        max_tokens: int,
        temperature: float,
        model: Optional[str]
    ) -> AsyncGenerator[str, None]:
        """OpenAIæµå¼ç”Ÿæˆ"""
        if not self.settings.ai_service.openai_api_key:
            raise ValueError("OpenAI APIå¯†é’¥æœªé…ç½®")

        session = await self._get_session()

        headers = {
            "Authorization": f"Bearer {self.settings.ai_service.openai_api_key}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream"
        }

        messages = []
        if context:
            messages.append({"role": "system", "content": context})
        messages.append({"role": "user", "content": prompt})

        data = {
            "model": model or self.settings.ai_service.openai_model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": True
        }

        url = f"{self.settings.ai_service.openai_base_url}/chat/completions"

        try:
            async with session.post(url, headers=headers, json=data) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"OpenAI APIé”™è¯¯: {response.status}, {error_text}")

                async for line in response.content:
                    line = line.decode('utf-8').strip()

                    if line.startswith('data: '):
                        data_str = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€

                        if data_str == '[DONE]':
                            break

                        try:
                            import json
                            data_obj = json.loads(data_str)

                            if 'choices' in data_obj and len(data_obj['choices']) > 0:
                                delta = data_obj['choices'][0].get('delta', {})
                                content = delta.get('content', '')

                                if content:
                                    yield content

                        except json.JSONDecodeError:
                            continue

        except Exception as e:
            logger.error(f"OpenAIæµå¼ç”Ÿæˆå¤±è´¥: {e}")
            raise Exception(f"OpenAIæµå¼æœåŠ¡ä¸å¯ç”¨: {str(e)}")
    
    async def _generate_stream_deepseek(
        self,
        prompt: str,
        context: str,
        max_tokens: int,
        temperature: float,
        model: Optional[str]
    ) -> AsyncGenerator[str, None]:
        """DeepSeekæµå¼ç”Ÿæˆ"""
        if not self.settings.ai_service.deepseek_api_key:
            raise ValueError("DeepSeek APIå¯†é’¥æœªé…ç½®")

        session = await self._get_session()

        headers = {
            "Authorization": f"Bearer {self.settings.ai_service.deepseek_api_key}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream"
        }

        messages = []
        if context:
            messages.append({"role": "system", "content": context})
        messages.append({"role": "user", "content": prompt})

        data = {
            "model": model or self.settings.ai_service.deepseek_model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": True
        }

        url = f"{self.settings.ai_service.deepseek_base_url}/chat/completions"

        try:
            async with session.post(url, headers=headers, json=data) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"DeepSeek APIé”™è¯¯: {response.status}, {error_text}")

                async for line in response.content:
                    line = line.decode('utf-8').strip()

                    if line.startswith('data: '):
                        data_str = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€

                        if data_str == '[DONE]':
                            break

                        try:
                            import json
                            data_obj = json.loads(data_str)

                            if 'choices' in data_obj and len(data_obj['choices']) > 0:
                                delta = data_obj['choices'][0].get('delta', {})
                                content = delta.get('content', '')

                                if content:
                                    yield content

                        except json.JSONDecodeError:
                            continue

        except Exception as e:
            logger.error(f"DeepSeekæµå¼ç”Ÿæˆå¤±è´¥: {e}")
            raise Exception(f"DeepSeekæµå¼æœåŠ¡ä¸å¯ç”¨: {str(e)}")
    

    
    async def analyze_text(
        self, 
        text: str, 
        analysis_type: str,
        model: Optional[str] = None
    ) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬"""
        try:
            if analysis_type == "style":
                return await self._analyze_style(text, model)
            elif analysis_type == "plot":
                return await self._analyze_plot(text, model)
            elif analysis_type == "character":
                return await self._analyze_character(text, model)
            else:
                return {"error": f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}"}
                
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def _analyze_style(self, text: str, model: Optional[str]) -> Dict[str, Any]:
        """é£æ ¼åˆ†æ"""
        # æ¨¡æ‹Ÿé£æ ¼åˆ†æç»“æœ
        await asyncio.sleep(0.5)
        
        return {
            "analysis_type": "style",
            "characteristics": [
                "æ–‡ç¬”æµç•…è‡ªç„¶",
                "æå†™ç»†è…»ç”ŸåŠ¨",
                "æƒ…æ„Ÿè¡¨è¾¾ä¸°å¯Œ",
                "è¯­è¨€ç®€æ´æ˜äº†"
            ],
            "tone": "æ¸©å’Œäº²åˆ‡",
            "complexity": "ä¸­ç­‰",
            "readability": "è‰¯å¥½",
            "suggestions": [
                "å¯ä»¥å¢åŠ ä¸€äº›ä¿®è¾æ‰‹æ³•",
                "é€‚å½“ä¸°å¯Œè¯æ±‡è¡¨è¾¾",
                "æ³¨æ„å¥å¼çš„å˜åŒ–"
            ]
        }
    
    async def _analyze_plot(self, text: str, model: Optional[str]) -> Dict[str, Any]:
        """æƒ…èŠ‚åˆ†æ"""
        # æ¨¡æ‹Ÿæƒ…èŠ‚åˆ†æç»“æœ
        await asyncio.sleep(0.5)
        
        return {
            "analysis_type": "plot",
            "structure": {
                "beginning": "å¼•äººå…¥èƒœçš„å¼€å¤´",
                "development": "æƒ…èŠ‚å‘å±•è‡ªç„¶",
                "climax": "é«˜æ½®éƒ¨åˆ†éœ€è¦åŠ å¼º",
                "resolution": "ç»“å°¾æœ‰å¾…å®Œå–„"
            },
            "pacing": "èŠ‚å¥é€‚ä¸­",
            "tension": "å¼ åŠ›é€‚åº¦",
            "suggestions": [
                "å¯ä»¥å¢åŠ æ›´å¤šå†²çª",
                "åŠ å¼ºæƒ…èŠ‚è½¬æŠ˜",
                "ä¸°å¯Œç»†èŠ‚æå†™"
            ]
        }
    
    async def _analyze_character(self, text: str, model: Optional[str]) -> Dict[str, Any]:
        """è§’è‰²åˆ†æ"""
        # æ¨¡æ‹Ÿè§’è‰²åˆ†æç»“æœ
        await asyncio.sleep(0.5)
        
        return {
            "analysis_type": "character",
            "development": "è§’è‰²å‘å±•è¾ƒå¥½",
            "consistency": "æ€§æ ¼ä¸€è‡´æ€§è‰¯å¥½",
            "depth": "äººç‰©æ·±åº¦é€‚ä¸­",
            "relationships": "è§’è‰²å…³ç³»æ¸…æ™°",
            "suggestions": [
                "å¯ä»¥å¢åŠ è§’è‰²èƒŒæ™¯",
                "ä¸°å¯Œè§’è‰²å†…å¿ƒæ´»åŠ¨",
                "åŠ å¼ºè§’è‰²å¯¹è¯ç‰¹è‰²"
            ]
        }
    
    async def improve_text(
        self, 
        text: str, 
        improvement_type: str,
        instructions: str = "",
        model: Optional[str] = None
    ) -> str:
        """æ”¹è¿›æ–‡æœ¬"""
        try:
            context = f"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œ{improvement_type}æ”¹è¿›ã€‚"
            if instructions:
                context += f" å…·ä½“è¦æ±‚ï¼š{instructions}"
            
            prompt = f"åŸæ–‡ï¼š\n{text}\n\næ”¹è¿›åçš„æ–‡æœ¬ï¼š"
            
            return await self.generate_text(prompt, context, model=model)
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æ”¹è¿›å¤±è´¥: {e}")
            return text  # è¿”å›åŸæ–‡
    
    async def check_availability(self, provider: str) -> bool:
        """æ£€æŸ¥ç‰¹å®šæä¾›å•†çš„AIæœåŠ¡å¯ç”¨æ€§"""
        try:
            if provider == "openai":
                return bool(self.settings.ai_service.openai_api_key)
            elif provider == "deepseek":
                return bool(self.settings.ai_service.deepseek_api_key)
            else:
                return True  # æ¨¡æ‹ŸæœåŠ¡æ€»æ˜¯å¯ç”¨

        except Exception as e:
            logger.error(f"æ£€æŸ¥AIæœåŠ¡å¯ç”¨æ€§å¤±è´¥: {e}")
            return False

    async def is_available(self) -> bool:
        """æ£€æŸ¥AIæœåŠ¡æ•´ä½“å¯ç”¨æ€§"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å¯ç”¨çš„æä¾›å•†
            openai_available = await self.check_availability("openai")
            deepseek_available = await self.check_availability("deepseek")

            return openai_available or deepseek_available

        except Exception as e:
            logger.error(f"æ£€æŸ¥AIæœåŠ¡æ•´ä½“å¯ç”¨æ€§å¤±è´¥: {e}")
            return False
    
    async def get_model_info(self, model: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        # æ”¯æŒçš„æ¨¡å‹ä¿¡æ¯
        model_info = {
            "gpt-3.5-turbo": {
                "name": "GPT-3.5 Turbo",
                "provider": "OpenAI",
                "max_tokens": 4096,
                "cost_per_1k_tokens": 0.002
            },
            "gpt-4": {
                "name": "GPT-4",
                "provider": "OpenAI",
                "max_tokens": 8192,
                "cost_per_1k_tokens": 0.03
            },
            "deepseek-chat": {
                "name": "DeepSeek Chat",
                "provider": "DeepSeek",
                "max_tokens": 4096,
                "cost_per_1k_tokens": 0.001
            }
        }
        
        return model_info.get(model, {"name": model, "provider": "Unknown"})
    
    async def list_available_models(self, provider: str) -> List[str]:
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        models = {
            "openai": ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"],
            "deepseek": ["deepseek-chat", "deepseek-coder"]
        }

        return models.get(provider, [])

    def reload_settings(self):
        """é‡æ–°åŠ è½½è®¾ç½®"""
        try:
            logger.info("é‡æ–°åŠ è½½AIä»“å‚¨è®¾ç½®...")

            # é‡æ–°åŠ è½½è®¾ç½®å¯¹è±¡
            from config.settings import get_settings
            self.settings = get_settings()

            # æ¸…ç†ç¼“å­˜
            self._response_cache.clear()

            # å…³é—­ç°æœ‰ä¼šè¯
            if self._session and not self._session.closed:
                asyncio.create_task(self._session.close())
                self._session = None

            # é‡ç½®å®¢æˆ·ç«¯ç®¡ç†å™¨
            self._client_manager = None

            logger.info("AIä»“å‚¨è®¾ç½®é‡æ–°åŠ è½½å®Œæˆ")

        except Exception as e:
            logger.error(f"é‡æ–°åŠ è½½AIä»“å‚¨è®¾ç½®å¤±è´¥: {e}")
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        if self._session and not self._session.closed:
            await self._session.close()

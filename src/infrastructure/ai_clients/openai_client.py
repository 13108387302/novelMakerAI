#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenAIå®¢æˆ·ç«¯å°è£…

ä½¿ç”¨å®˜æ–¹OpenAI Pythonå®¢æˆ·ç«¯å®ç°æ›´ç¨³å®šçš„AIæœåŠ¡
"""

import asyncio
from typing import AsyncGenerator, List, Dict, Any, Optional
import openai
import httpx
from openai import AsyncOpenAI
from functools import wraps

from config.settings import get_settings
from src.shared.utils.logger import get_logger
from src.shared.utils.error_handler import handle_async_errors, ApplicationError

logger = get_logger(__name__)


def retry_on_timeout(max_retries: int = 3, delay: float = 1.0):
    """
    è¶…æ—¶é‡è¯•è£…é¥°å™¨

    å½“é‡åˆ°è¶…æ—¶é”™è¯¯æ—¶è‡ªåŠ¨é‡è¯•ï¼Œæ”¯æŒæŒ‡æ•°é€€é¿å’Œç½‘ç»œçŠ¶æ€æ£€æµ‹
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            from src.shared.utils.network_utils import smart_retry_with_backoff

            # ä½¿ç”¨æ™ºèƒ½é‡è¯•æœºåˆ¶
            async def execute_func():
                return await func(*args, **kwargs)

            return await smart_retry_with_backoff(
                execute_func,
                max_retries=max_retries,
                base_delay=delay
            )

        return wrapper
    return decorator


class AIClientError(ApplicationError):
    """
    AIå®¢æˆ·ç«¯é”™è¯¯åŸºç±»

    æ‰€æœ‰AIå®¢æˆ·ç«¯ç›¸å…³é”™è¯¯çš„åŸºç±»ï¼Œç»§æ‰¿è‡ªApplicationErrorã€‚
    """
    pass


class AIServiceUnavailableError(AIClientError):
    """
    AIæœåŠ¡ä¸å¯ç”¨é”™è¯¯

    å½“AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨æˆ–ç½‘ç»œè¿æ¥å¤±è´¥æ—¶æŠ›å‡ºã€‚
    """
    pass


class AIQuotaExceededError(AIClientError):
    """
    AIé…é¢è¶…é™é”™è¯¯

    å½“APIè°ƒç”¨è¶…å‡ºé…é¢é™åˆ¶æ—¶æŠ›å‡ºã€‚
    """
    pass


class AIRequestTimeoutError(AIClientError):
    """
    AIè¯·æ±‚è¶…æ—¶é”™è¯¯

    å½“AIè¯·æ±‚è¶…è¿‡è®¾å®šçš„è¶…æ—¶æ—¶é—´æ—¶æŠ›å‡ºã€‚
    """
    pass


# å…¨å±€è®¾ç½®æœåŠ¡å®ä¾‹ï¼ˆç”±ä¸»åº”ç”¨è®¾ç½®ï¼‰
_global_settings_service = None

def set_global_settings_service(settings_service):
    """è®¾ç½®å…¨å±€è®¾ç½®æœåŠ¡å®ä¾‹"""
    global _global_settings_service
    _global_settings_service = settings_service
    logger.info("å…¨å±€è®¾ç½®æœåŠ¡å·²è®¾ç½®")

def get_ai_config():
    """è·å–AIé…ç½®ï¼ˆä»è®¾ç½®æœåŠ¡æˆ–å…¨å±€è®¾ç½®ï¼‰"""
    global _global_settings_service

    # å°è¯•ä»å…¨å±€è®¾ç½®æœåŠ¡è·å–
    if _global_settings_service:
        try:
            return {
                'openai_api_key': _global_settings_service.get_setting('ai.openai_api_key', ''),
                'openai_base_url': _global_settings_service.get_setting('ai.openai_base_url', 'https://api.openai.com/v1'),
                'openai_model': _global_settings_service.get_setting('ai.openai_model', 'gpt-3.5-turbo'),
                'deepseek_api_key': _global_settings_service.get_setting('ai.deepseek_api_key', ''),
                'deepseek_base_url': _global_settings_service.get_setting('ai.deepseek_base_url', 'https://api.deepseek.com/v1'),
                'deepseek_model': _global_settings_service.get_setting('ai.deepseek_model', 'deepseek-chat'),
                'default_provider': _global_settings_service.get_setting('ai.default_provider', 'openai'),
                'timeout': _global_settings_service.get_setting('ai.timeout', 120),
                'max_tokens': _global_settings_service.get_setting('ai.max_tokens', 2000),
                'temperature': _global_settings_service.get_setting('ai.temperature', 0.7),
            }
        except Exception as e:
            logger.warning(f"ä»è®¾ç½®æœåŠ¡è·å–AIé…ç½®å¤±è´¥ï¼Œä½¿ç”¨å…¨å±€è®¾ç½®: {e}")
    else:
        logger.debug("å…¨å±€è®¾ç½®æœåŠ¡æœªè®¾ç½®ï¼Œä½¿ç”¨å…¨å±€è®¾ç½®")

    # é™çº§åˆ°å…¨å±€è®¾ç½®
    try:
        from config.settings import get_settings
        settings = get_settings()
        return {
            'openai_api_key': settings.ai_service.openai_api_key or '',
            'openai_base_url': settings.ai_service.openai_base_url,
            'openai_model': settings.ai_service.openai_model,
            'deepseek_api_key': settings.ai_service.deepseek_api_key or '',
            'deepseek_base_url': settings.ai_service.deepseek_base_url,
            'deepseek_model': settings.ai_service.deepseek_model,
            'default_provider': settings.ai_service.default_provider,
            'timeout': settings.ai_service.timeout,
            'max_tokens': settings.ai_service.max_tokens,
            'temperature': settings.ai_service.temperature,
        }
    except Exception as e:
        logger.error(f"è·å–å…¨å±€è®¾ç½®å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤é…ç½®
        return {
            'openai_api_key': '',
            'openai_base_url': 'https://api.openai.com/v1',
            'openai_model': 'gpt-3.5-turbo',
            'deepseek_api_key': '',
            'deepseek_base_url': 'https://api.deepseek.com/v1',
            'deepseek_model': 'deepseek-chat',
            'default_provider': 'openai',
            'timeout': 120,
            'max_tokens': 2000,
            'temperature': 0.7,
        }

    # é™çº§åˆ°å…¨å±€è®¾ç½®
    settings = get_settings()
    return {
        'openai_api_key': settings.ai_service.openai_api_key or '',
        'openai_base_url': settings.ai_service.openai_base_url,
        'openai_model': settings.ai_service.openai_model,
        'deepseek_api_key': settings.ai_service.deepseek_api_key or '',
        'deepseek_base_url': settings.ai_service.deepseek_base_url,
        'deepseek_model': settings.ai_service.deepseek_model,
        'default_provider': settings.ai_service.default_provider,
        'timeout': settings.ai_service.timeout,
        'max_tokens': settings.ai_service.max_tokens,
        'temperature': settings.ai_service.temperature,
    }


class OpenAIStreamingClient:
    """
    OpenAIæµå¼å®¢æˆ·ç«¯ - ä¼˜åŒ–ç‰ˆæœ¬

    å°è£…OpenAI APIçš„æµå¼è°ƒç”¨åŠŸèƒ½ï¼Œæ”¯æŒå¤šä¸ªAIæœåŠ¡æä¾›å•†ã€‚
    æä¾›ç»Ÿä¸€çš„æ¥å£å¤„ç†æ–‡æœ¬ç”Ÿæˆå’Œæµå¼å“åº”ã€‚

    ä¼˜åŒ–åŠŸèƒ½ï¼š
    - è¿æ¥æ± ç®¡ç†å’Œå¤ç”¨
    - æ™ºèƒ½é‡è¯•å’Œæ•…éšœè½¬ç§»
    - æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡
    - è‡ªé€‚åº”è¶…æ—¶å’Œé™æµ
    - è¯·æ±‚ç¼“å­˜æœºåˆ¶

    Attributes:
        config: AIæœåŠ¡é…ç½®å­—å…¸
        _clients: å®¢æˆ·ç«¯è¿æ¥æ± 
        _client_stats: å®¢æˆ·ç«¯ç»Ÿè®¡ä¿¡æ¯
        _request_cache: è¯·æ±‚ç¼“å­˜
    """

    def __init__(self):
        """
        åˆå§‹åŒ–OpenAIæµå¼å®¢æˆ·ç«¯

        åŠ è½½AIé…ç½®å¹¶åˆå§‹åŒ–å®¢æˆ·ç«¯è¿æ¥æ± ã€‚
        """
        self.config = get_ai_config()

        # è¿æ¥æ± ç®¡ç†
        self._clients: Dict[str, AsyncOpenAI] = {}
        self._client_locks: Dict[str, asyncio.Lock] = {}

        # æ€§èƒ½ç»Ÿè®¡
        self._client_stats: Dict[str, Dict[str, Any]] = {}
        self._request_count = 0
        self._success_count = 0
        self._error_count = 0

        # ç¼“å­˜ç®¡ç†
        self._request_cache: Dict[str, Any] = {}
        self._cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

        # é™æµæ§åˆ¶
        self._rate_limiters: Dict[str, asyncio.Semaphore] = {}

        logger.info("OpenAIæµå¼å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰")

    def reload_settings(self):
        """
        é‡æ–°åŠ è½½è®¾ç½®å¹¶é‡ç½®å®¢æˆ·ç«¯è¿æ¥æ± 

        å½“é…ç½®å‘ç”Ÿå˜åŒ–æ—¶ï¼Œé‡æ–°åŠ è½½é…ç½®å¹¶é‡ç½®æ‰€æœ‰å®¢æˆ·ç«¯å®ä¾‹ã€‚
        ç”¨äºåŠ¨æ€æ›´æ–°APIå¯†é’¥å’Œå…¶ä»–é…ç½®ã€‚
        """
        self.config = get_ai_config()

        # æ¸…ç†ç°æœ‰è¿æ¥
        self._clients.clear()
        self._client_locks.clear()
        self._rate_limiters.clear()

        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self._client_stats.clear()
        self._request_cache.clear()

        logger.info("AIå®¢æˆ·ç«¯è®¾ç½®å·²é‡æ–°åŠ è½½ï¼ˆè¿æ¥æ± å·²é‡ç½®ï¼‰")
    
    async def _get_client(self, provider: str) -> AsyncOpenAI:
        """è·å–å®¢æˆ·ç«¯å®ä¾‹ - è¿æ¥æ± ç‰ˆæœ¬"""
        # ç¡®ä¿æœ‰é”
        if provider not in self._client_locks:
            self._client_locks[provider] = asyncio.Lock()

        # ç¡®ä¿æœ‰é™æµå™¨
        if provider not in self._rate_limiters:
            # æ¯ä¸ªæä¾›å•†æœ€å¤šåŒæ—¶5ä¸ªè¯·æ±‚
            self._rate_limiters[provider] = asyncio.Semaphore(5)

        async with self._client_locks[provider]:
            if provider not in self._clients:
                self._clients[provider] = await self._create_client(provider)

                # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
                self._client_stats[provider] = {
                    'requests': 0,
                    'successes': 0,
                    'failures': 0,
                    'avg_response_time': 0.0,
                    'last_used': None
                }

        return self._clients[provider]

    async def _create_client(self, provider: str) -> AsyncOpenAI:
        """åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯å®ä¾‹"""
        if provider.lower() == 'openai':
            api_key = self.config.get('openai_api_key', '').strip()
            base_url = self.config.get('openai_base_url', 'https://api.openai.com/v1')
        elif provider.lower() == 'deepseek':
            api_key = self.config.get('deepseek_api_key', '').strip()
            base_url = self.config.get('deepseek_base_url', 'https://api.deepseek.com/v1')
        else:
            raise AIClientError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}")

        if not api_key:
            raise AIClientError(f"{provider} APIå¯†é’¥æœªé…ç½®")

        try:
            # ä½¿ç”¨è‡ªé€‚åº”è¶…æ—¶
            from src.shared.utils.network_utils import get_adaptive_timeout
            adaptive_timeout = get_adaptive_timeout()
            final_timeout = max(self.config.get('timeout', 120), adaptive_timeout)

            # åˆ›å»ºHTTPå®¢æˆ·ç«¯é…ç½®
            http_client = httpx.AsyncClient(
                timeout=httpx.Timeout(final_timeout),
                limits=httpx.Limits(
                    max_keepalive_connections=10,
                    max_connections=20,
                    keepalive_expiry=30.0
                )
            )

            client = AsyncOpenAI(
                api_key=api_key,
                base_url=base_url,
                http_client=http_client
            )

            logger.info(f"{provider}å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸï¼Œè¶…æ—¶è®¾ç½®: {final_timeout:.1f}ç§’")
            return client

        except Exception as e:
            raise AIClientError(f"åˆ›å»º{provider}å®¢æˆ·ç«¯å¤±è´¥: {e}")

    def _generate_cache_key(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        import json

        # æ„å»ºç¼“å­˜å†…å®¹
        cache_content = {
            'messages': messages,
            'model': kwargs.get('model'),
            'max_tokens': kwargs.get('max_tokens'),
            'temperature': kwargs.get('temperature')
        }

        # ç”Ÿæˆå“ˆå¸Œ
        content_str = json.dumps(cache_content, sort_keys=True)
        return hashlib.md5(content_str.encode()).hexdigest()

    def _get_cached_response(self, cache_key: str) -> Optional[str]:
        """è·å–ç¼“å­˜å“åº”"""
        if cache_key in self._request_cache:
            cached_item = self._request_cache[cache_key]
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - cached_item['timestamp'] < self._cache_ttl:
                return cached_item['response']
            else:
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                del self._request_cache[cache_key]
        return None

    def _cache_response(self, cache_key: str, response: str):
        """ç¼“å­˜å“åº”"""
        self._request_cache[cache_key] = {
            'response': response,
            'timestamp': time.time()
        }

        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self._request_cache) > 100:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = min(
                self._request_cache.keys(),
                key=lambda k: self._request_cache[k]['timestamp']
            )
            del self._request_cache[oldest_key]
    
    async def stream_chat_completion(
        self,
        messages: List[Dict[str, str]],
        provider: str = "openai",
        model: Optional[str] = None,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """æµå¼èŠå¤©å®Œæˆ"""
        start_time = time.time()

        # è·å–é™æµå™¨
        rate_limiter = self._rate_limiters.get(provider)
        if not rate_limiter:
            rate_limiter = asyncio.Semaphore(5)
            self._rate_limiters[provider] = rate_limiter

        async with rate_limiter:
            try:
                # è·å–å®¢æˆ·ç«¯
                client = await self._get_client(provider)

                # ç¡®å®šæ¨¡å‹
                if provider.lower() == "openai":
                    model = model or self.config.get('openai_model', 'gpt-3.5-turbo')
                elif provider.lower() == "deepseek":
                    model = model or self.config.get('deepseek_model', 'deepseek-chat')
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {provider}")

                logger.info(f"å¼€å§‹æµå¼ç”Ÿæˆï¼Œæä¾›å•†: {provider}, æ¨¡å‹: {model}")

                # æ›´æ–°ç»Ÿè®¡
                self._request_count += 1
                stats = self._client_stats.get(provider, {})
                stats['requests'] = stats.get('requests', 0) + 1
                stats['last_used'] = datetime.now()

                # åˆ›å»ºæµå¼è¯·æ±‚
                response = await client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stream=True,
                    **kwargs
                )

                # å¤„ç†æµå¼å“åº”
                chunk_count = 0
                try:
                    async for chunk in response:
                        if chunk.choices and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            if hasattr(delta, 'content') and delta.content:
                                chunk_count += 1
                                yield delta.content

                except Exception as stream_error:
                    logger.error(f"å¤„ç†æµå¼å“åº”å¤±è´¥: {stream_error}")
                    # å¦‚æœæµå¼å¤„ç†å¤±è´¥ï¼Œå°è¯•é™çº§åˆ°éæµå¼
                    logger.warning("æµå¼å¤„ç†å¤±è´¥ï¼Œé™çº§åˆ°éæµå¼ç”Ÿæˆ")

                    # é‡æ–°åˆ›å»ºéæµå¼è¯·æ±‚
                    fallback_response = await client.chat.completions.create(
                        model=model,
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        stream=False,
                        **kwargs
                    )

                    # åˆ†å—è¿”å›éæµå¼ç»“æœ
                    if fallback_response.choices and len(fallback_response.choices) > 0:
                        content = fallback_response.choices[0].message.content
                        if content:
                            # å°†å†…å®¹åˆ†æˆå°å—æ¨¡æ‹Ÿæµå¼è¾“å‡º
                            chunk_size = 10
                            for i in range(0, len(content), chunk_size):
                                chunk_count += 1
                                yield content[i:i + chunk_size]

                # è®°å½•æˆåŠŸ
                processing_time = time.time() - start_time
                self._success_count += 1
                stats['successes'] = stats.get('successes', 0) + 1

                # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
                if 'avg_response_time' not in stats:
                    stats['avg_response_time'] = processing_time
                else:
                    stats['avg_response_time'] = (
                        stats['avg_response_time'] * 0.7 + processing_time * 0.3
                    )

                logger.info(f"æµå¼ç”Ÿæˆå®Œæˆ - å—æ•°: {chunk_count}, è€—æ—¶: {processing_time:.2f}ç§’")

            except openai.APIError as e:
                # è®°å½•é”™è¯¯
                self._error_count += 1
                stats = self._client_stats.get(provider, {})
                stats['failures'] = stats.get('failures', 0) + 1

                logger.error(f"OpenAI APIé”™è¯¯: {e}")
                raise AIClientError(f"APIè°ƒç”¨å¤±è´¥: {e}")
            except openai.APIConnectionError as e:
                # è®°å½•é”™è¯¯
                self._error_count += 1
                stats = self._client_stats.get(provider, {})
                stats['failures'] = stats.get('failures', 0) + 1

                logger.error(f"OpenAIè¿æ¥é”™è¯¯: {e}")
                raise AIServiceUnavailableError(f"æ— æ³•è¿æ¥åˆ°AIæœåŠ¡: {e}")
            except openai.RateLimitError as e:
                # è®°å½•é”™è¯¯
                self._error_count += 1
                stats = self._client_stats.get(provider, {})
                stats['failures'] = stats.get('failures', 0) + 1

                logger.error(f"OpenAIé€Ÿç‡é™åˆ¶: {e}")
                raise AIQuotaExceededError(f"APIè°ƒç”¨é¢‘ç‡è¶…é™: {e}")
            except (asyncio.TimeoutError, httpx.ReadTimeout, httpx.TimeoutException) as e:
                # è®°å½•é”™è¯¯
                self._error_count += 1
                stats = self._client_stats.get(provider, {})
                stats['failures'] = stats.get('failures', 0) + 1

                logger.error(f"è¯·æ±‚è¶…æ—¶: {e}")
                raise AIRequestTimeoutError("AIæœåŠ¡å“åº”è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
            except Exception as e:
                # è®°å½•é”™è¯¯
                self._error_count += 1
                stats = self._client_stats.get(provider, {})
                stats['failures'] = stats.get('failures', 0) + 1

                logger.error(f"æµå¼ç”Ÿæˆå¤±è´¥: {e}")
                raise AIClientError(f"æœªçŸ¥é”™è¯¯: {e}")
    
    async def simple_chat_completion(
        self,
        messages: List[Dict[str, str]],
        provider: str = "openai",
        model: Optional[str] = None,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """ç®€å•èŠå¤©å®Œæˆï¼ˆéæµå¼ï¼‰"""
        try:
            if provider.lower() == "openai":
                client = await self._get_client("openai")
                model = model or self.config.get('openai_model', 'gpt-3.5-turbo')
            elif provider.lower() == "deepseek":
                client = await self._get_client("deepseek")
                model = model or self.config.get('deepseek_model', 'deepseek-chat')
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {provider}")
            
            response = await client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                stream=False,
                **kwargs
            )
            
            if response.choices and len(response.choices) > 0:
                return response.choices[0].message.content
            else:
                raise AIClientError("AIå“åº”ä¸ºç©º")
                
        except openai.APIError as e:
            logger.error(f"OpenAI APIé”™è¯¯: {e}")
            raise AIClientError(f"APIè°ƒç”¨å¤±è´¥: {e}")
        except openai.APIConnectionError as e:
            logger.error(f"OpenAIè¿æ¥é”™è¯¯: {e}")
            raise AIServiceUnavailableError(f"æ— æ³•è¿æ¥åˆ°AIæœåŠ¡: {e}")
        except openai.RateLimitError as e:
            logger.error(f"OpenAIé€Ÿç‡é™åˆ¶: {e}")
            raise AIQuotaExceededError(f"APIè°ƒç”¨é¢‘ç‡è¶…é™: {e}")
        except (asyncio.TimeoutError, httpx.ReadTimeout, httpx.TimeoutException) as e:
            logger.error(f"è¯·æ±‚è¶…æ—¶: {e}")
            raise AIRequestTimeoutError("AIæœåŠ¡å“åº”è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
        except Exception as e:
            logger.error(f"èŠå¤©å®Œæˆå¤±è´¥: {e}")
            raise AIClientError(f"æœªçŸ¥é”™è¯¯: {e}")
    
    async def stream_with_prompt(
        self,
        prompt: str,
        context: str = "",
        provider: str = "openai",
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨æç¤ºè¯è¿›è¡Œæµå¼ç”Ÿæˆ"""
        messages = []
        
        if context:
            messages.append({"role": "system", "content": context})
        
        messages.append({"role": "user", "content": prompt})
        
        async for chunk in self.stream_chat_completion(
            messages=messages,
            provider=provider,
            **kwargs
        ):
            yield chunk
    
    async def complete_with_prompt(
        self,
        prompt: str,
        context: str = "",
        provider: str = "openai",
        **kwargs
    ) -> str:
        """ä½¿ç”¨æç¤ºè¯è¿›è¡Œå®Œæˆï¼ˆéæµå¼ï¼‰"""
        messages = []
        
        if context:
            messages.append({"role": "system", "content": context})
        
        messages.append({"role": "user", "content": prompt})
        
        return await self.simple_chat_completion(
            messages=messages,
            provider=provider,
            **kwargs
        )


class AIClientManager:
    """AIå®¢æˆ·ç«¯ç®¡ç†å™¨"""

    def __init__(self):
        self.openai_client = OpenAIStreamingClient()
        self.config = get_ai_config()

    def _select_available_provider(self) -> str:
        """æ™ºèƒ½é€‰æ‹©å¯ç”¨çš„AIæä¾›å•†"""
        # è·å–é»˜è®¤æä¾›å•†
        default_provider = self.config.get('default_provider', 'deepseek')

        # æ£€æŸ¥é»˜è®¤æä¾›å•†çš„APIå¯†é’¥æ˜¯å¦å¯ç”¨
        default_api_key = self.config.get(f"{default_provider}_api_key")
        if default_api_key:
            logger.info(f"ğŸ¯ ä½¿ç”¨é»˜è®¤æä¾›å•†: {default_provider}")
            return default_provider

        # å¦‚æœé»˜è®¤æä¾›å•†ä¸å¯ç”¨ï¼Œå°è¯•å…¶ä»–æä¾›å•†
        providers = ['deepseek', 'openai']
        for provider in providers:
            if provider != default_provider:
                api_key = self.config.get(f"{provider}_api_key")
                if api_key:
                    logger.info(f"ğŸ”„ é»˜è®¤æä¾›å•† {default_provider} ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°: {provider}")
                    return provider

        # å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤æä¾›å•†ï¼ˆä¼šåœ¨åç»­æ£€æŸ¥ä¸­æŠ¥é”™ï¼‰
        logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„APIå¯†é’¥ï¼Œä½¿ç”¨é»˜è®¤æä¾›å•†: {default_provider}")
        return default_provider

    def reload_settings(self):
        """é‡æ–°åŠ è½½è®¾ç½®é…ç½®"""
        logger.info("ğŸ”„ é‡æ–°åŠ è½½AIå®¢æˆ·ç«¯è®¾ç½®...")
        old_config = self.config.copy()
        self.config = get_ai_config()

        # æ¯”è¾ƒé…ç½®å˜åŒ–
        changes = []
        if old_config.get('default_provider') != self.config.get('default_provider'):
            changes.append(f"é»˜è®¤æä¾›å•†: {old_config.get('default_provider')} â†’ {self.config.get('default_provider')}")

        for provider in ['openai', 'deepseek']:
            old_key = old_config.get(f'{provider}_api_key')
            new_key = self.config.get(f'{provider}_api_key')
            if bool(old_key) != bool(new_key):
                status = "å·²é…ç½®" if new_key else "å·²ç§»é™¤"
                changes.append(f"{provider} APIå¯†é’¥: {status}")

        if changes:
            logger.info("ğŸ“‹ é…ç½®å˜åŒ–:")
            for change in changes:
                logger.info(f"  - {change}")
        else:
            logger.info("ğŸ“‹ é…ç½®æ— å˜åŒ–")

        logger.info("âœ… AIå®¢æˆ·ç«¯è®¾ç½®é‡æ–°åŠ è½½å®Œæˆ")

    def reload_settings(self):
        """é‡æ–°åŠ è½½è®¾ç½®"""
        self.config = get_ai_config()
        self.openai_client.reload_settings()
        logger.info("AIå®¢æˆ·ç«¯ç®¡ç†å™¨è®¾ç½®å·²é‡æ–°åŠ è½½")
    
    @retry_on_timeout(max_retries=3, delay=3.0)
    async def stream_generate(
        self,
        prompt: str,
        context: str = "",
        max_tokens: int = 2000,
        temperature: float = 0.7,
        model: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆï¼ˆè‡ªåŠ¨é€‰æ‹©æä¾›å•†ï¼‰"""
        provider = self._select_available_provider()
        
        try:
            async for chunk in self.openai_client.stream_with_prompt(
                prompt=prompt,
                context=context,
                provider=provider,
                model=model,
                max_tokens=max_tokens,
                temperature=temperature
            ):
                yield chunk
                
        except Exception as e:
            logger.error(f"ä½¿ç”¨{provider}æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            
            # å°è¯•é™çº§åˆ°å¦ä¸€ä¸ªæä¾›å•†
            fallback_provider = "deepseek" if provider == "openai" else "openai"
            
            try:
                logger.info(f"å°è¯•é™çº§åˆ°{fallback_provider}")
                async for chunk in self.openai_client.stream_with_prompt(
                    prompt=prompt,
                    context=context,
                    provider=fallback_provider,
                    model=model,
                    max_tokens=max_tokens,
                    temperature=temperature
                ):
                    yield chunk
                    
            except Exception as fallback_error:
                logger.error(f"é™çº§åˆ°{fallback_provider}ä¹Ÿå¤±è´¥: {fallback_error}")
                raise Exception(f"æ‰€æœ‰AIæœåŠ¡éƒ½ä¸å¯ç”¨: ä¸»æœåŠ¡ - {str(e)}, å¤‡ç”¨æœåŠ¡ - {str(fallback_error)}")
    
    @retry_on_timeout(max_retries=3, delay=3.0)
    async def simple_generate(
        self,
        prompt: str,
        context: str = "",
        max_tokens: int = 2000,
        temperature: float = 0.7,
        model: Optional[str] = None
    ) -> str:
        """ç®€å•ç”Ÿæˆï¼ˆéæµå¼ï¼‰"""
        logger.info(f"ğŸ¯ AIå®¢æˆ·ç«¯ç®¡ç†å™¨å¼€å§‹ç®€å•ç”Ÿæˆï¼Œæç¤ºè¯é•¿åº¦: {len(prompt)}")

        # æ™ºèƒ½é€‰æ‹©å¯ç”¨çš„æä¾›å•†
        provider = self._select_available_provider()
        logger.debug(f"ä½¿ç”¨æä¾›å•†: {provider}")
        logger.debug(f"é…ç½®ä¿¡æ¯: max_tokens={max_tokens}, temperature={temperature}, model={model}")

        # æ£€æŸ¥APIå¯†é’¥
        api_key_key = f"{provider}_api_key"
        api_key = self.config.get(api_key_key)
        if not api_key:
            logger.error(f"âŒ {provider} APIå¯†é’¥æœªé…ç½®")
            raise Exception(f"{provider} APIå¯†é’¥æœªé…ç½®")
        else:
            logger.debug(f"âœ… {provider} APIå¯†é’¥å·²é…ç½®: {api_key[:10]}...")

        try:
            logger.info(f"ğŸ“¡ è°ƒç”¨ {provider} å®¢æˆ·ç«¯...")
            result = await self.openai_client.complete_with_prompt(
                prompt=prompt,
                context=context,
                provider=provider,
                model=model,
                max_tokens=max_tokens,
                temperature=temperature
            )

            logger.info(f"âœ… {provider} ç”ŸæˆæˆåŠŸï¼Œå“åº”é•¿åº¦: {len(result) if result else 0}")
            return result
            
        except Exception as e:
            logger.error(f"ä½¿ç”¨{provider}ç”Ÿæˆå¤±è´¥: {e}")
            
            # å°è¯•é™çº§åˆ°å¦ä¸€ä¸ªæä¾›å•†
            fallback_provider = "deepseek" if provider == "openai" else "openai"
            
            try:
                logger.info(f"å°è¯•é™çº§åˆ°{fallback_provider}")
                return await self.openai_client.complete_with_prompt(
                    prompt=prompt,
                    context=context,
                    provider=fallback_provider,
                    model=model,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
            except Exception as fallback_error:
                logger.error(f"é™çº§åˆ°{fallback_provider}ä¹Ÿå¤±è´¥: {fallback_error}")
                raise Exception(f"æ‰€æœ‰AIæœåŠ¡éƒ½ä¸å¯ç”¨: ä¸»æœåŠ¡ - {str(e)}, å¤‡ç”¨æœåŠ¡ - {str(fallback_error)}")
    



# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_ai_client_manager = None

def get_ai_client_manager() -> AIClientManager:
    """è·å–AIå®¢æˆ·ç«¯ç®¡ç†å™¨å®ä¾‹"""
    global _ai_client_manager
    if _ai_client_manager is None:
        _ai_client_manager = AIClientManager()
    return _ai_client_manager

def reload_ai_client_settings():
    """é‡æ–°åŠ è½½AIå®¢æˆ·ç«¯è®¾ç½®"""
    global _ai_client_manager
    if _ai_client_manager is not None:
        _ai_client_manager.reload_settings()
    else:
        # å¦‚æœè¿˜æ²¡æœ‰åˆ›å»ºå®ä¾‹ï¼Œä¸‹æ¬¡åˆ›å»ºæ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨æ–°è®¾ç½®
        logger.info("AIå®¢æˆ·ç«¯ç®¡ç†å™¨å°šæœªåˆ›å»ºï¼Œå°†åœ¨ä¸‹æ¬¡ä½¿ç”¨æ—¶åº”ç”¨æ–°è®¾ç½®")


# ä¸ºOpenAIStreamingClientæ·»åŠ æ€§èƒ½ç›‘æ§æ–¹æ³•
def add_performance_methods_to_client():
    """ä¸ºOpenAIStreamingClientæ·»åŠ æ€§èƒ½ç›‘æ§æ–¹æ³•"""

    def get_client_statistics(self) -> Dict[str, Any]:
        """è·å–å®¢æˆ·ç«¯ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_requests': getattr(self, '_request_count', 0),
            'total_successes': getattr(self, '_success_count', 0),
            'total_errors': getattr(self, '_error_count', 0),
            'success_rate': getattr(self, '_success_count', 0) / max(getattr(self, '_request_count', 1), 1),
            'active_clients': len(getattr(self, '_clients', {})),
            'cache_size': len(getattr(self, '_request_cache', {})),
            'provider_stats': dict(getattr(self, '_client_stats', {}))
        }

    def get_provider_health(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æä¾›å•†å¥åº·çŠ¶æ€"""
        health_info = {}
        client_stats = getattr(self, '_client_stats', {})

        for provider, stats in client_stats.items():
            if stats.get('requests', 0) > 0:
                success_rate = stats.get('successes', 0) / stats['requests']
                health_info[provider] = {
                    'is_healthy': success_rate >= 0.8,
                    'success_rate': success_rate,
                    'avg_response_time': stats.get('avg_response_time', 0),
                    'total_requests': stats.get('requests', 0),
                    'last_used': stats.get('last_used')
                }

        return health_info

    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        if hasattr(self, '_request_cache'):
            self._request_cache.clear()
            logger.info("AIå®¢æˆ·ç«¯ç¼“å­˜å·²æ¸…ç†")

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self._request_count = 0
        self._success_count = 0
        self._error_count = 0
        if hasattr(self, '_client_stats'):
            self._client_stats.clear()
        logger.info("AIå®¢æˆ·ç«¯ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")

    # åŠ¨æ€æ·»åŠ æ–¹æ³•åˆ°ç±»
    OpenAIStreamingClient.get_client_statistics = get_client_statistics
    OpenAIStreamingClient.get_provider_health = get_provider_health
    OpenAIStreamingClient.clear_cache = clear_cache
    OpenAIStreamingClient.reset_statistics = reset_statistics

# è°ƒç”¨å‡½æ•°æ·»åŠ æ–¹æ³•
add_performance_methods_to_client()
